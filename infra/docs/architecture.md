# ğŸ—ï¸ Architecture DOCKERWARTS - Documentation dÃ©taillÃ©e

## Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture rÃ©seau](#architecture-rÃ©seau)
3. [Architecture des services](#architecture-des-services)
4. [Flux de donnÃ©es](#flux-de-donnÃ©es)
5. [Gestion de la persistance](#gestion-de-la-persistance)
6. [Haute disponibilitÃ©](#haute-disponibilitÃ©)

---

## Vue d'ensemble

L'infrastructure DOCKERWARTS est construite selon une architecture microservices avec isolation rÃ©seau et rÃ©partition des responsabilitÃ©s.

### Principes architecturaux

1. **SÃ©paration des prÃ©occupations** : Frontend/Backend sur rÃ©seaux distincts
2. **Isolation** : Services groupÃ©s par fonction
3. **ScalabilitÃ©** : Architecture multi-nÅ“uds pour services critiques
4. **ObservabilitÃ©** : Monitoring Ã  tous les niveaux
5. **RÃ©silience** : Health checks et restart automatique

---

## Architecture rÃ©seau

### Topologie

```
Internet
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HOST MACHINE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Docker Bridge Network                  â”‚ â”‚
â”‚  â”‚                                                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚     FRONTEND NETWORK (172.20.0.0/24)      â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                             â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Traefik â”‚  â”‚ Grafana â”‚  â”‚  GLPI   â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :80    â”‚  â”‚  :3000  â”‚  â”‚  :80    â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :443   â”‚  â”‚         â”‚  â”‚         â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :8080  â”‚  â”‚         â”‚  â”‚         â”‚   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚       â”‚            â”‚            â”‚         â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚          â”‚            â”‚            â”‚              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚       â”‚   BACKEND NETWORK (172.21.0.0/24) â”‚   â”‚ â”‚
â”‚  â”‚  â”‚       â”‚            â”‚            â”‚          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚Promeths â”‚  â”‚   ES   â”‚  â”‚ GLPI DB  â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :9090  â”‚  â”‚  :9200 â”‚  â”‚  :3306   â”‚  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Cassandra â”‚  â”‚ Cassandra â”‚          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  Node 1   â”‚  â”‚  Node 2   â”‚          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :9042    â”‚  â”‚  :9042    â”‚          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                          â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚   Node   â”‚  â”‚ cAdvisor â”‚           â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ Exporter â”‚  â”‚  :8080   â”‚           â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  :9100   â”‚  â”‚          â”‚           â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RÃ©seaux Docker

#### Frontend Network (`172.20.0.0/24`)
- **Type** : Bridge
- **RÃ´le** : Services exposÃ©s aux utilisateurs
- **Services** :
  - Traefik (reverse proxy)
  - Grafana (monitoring UI)
  - Kibana (logs UI)
  - GLPI (ticketing UI)

#### Backend Network (`172.21.0.0/24`)
- **Type** : Bridge
- **RÃ´le** : Services internes et bases de donnÃ©es
- **Services** :
  - Prometheus (mÃ©triques)
  - Elasticsearch (cluster)
  - Cassandra (cluster)
  - MariaDB (base GLPI)
  - Node Exporter (mÃ©triques systÃ¨me)
  - cAdvisor (mÃ©triques containers)

### Ports exposÃ©s sur l'hÃ´te

| Service | Port | Protocole | Usage |
|---------|------|-----------|-------|
| Traefik | 80 | HTTP | EntrÃ©e principale |
| Traefik | 443 | HTTPS | EntrÃ©e principale (SSL) |
| Traefik | 8080 | HTTP | Dashboard Traefik |
| Elasticsearch | 9200 | HTTP | API REST (dÃ©veloppement) |

---

## Architecture des services

### Couche Proxy & Firewall

#### Traefik
```yaml
Role: Reverse Proxy + Application Firewall
Function:
  - Routage par domaine (Host-based routing)
  - Terminaison SSL/TLS
  - Load balancing
  - MÃ©triques pour Prometheus
Health Check: traefik healthcheck --ping
Restart: unless-stopped
```

**Justification du choix** :
- Configuration dynamique via labels Docker
- IntÃ©gration native avec Docker
- Dashboard intÃ©grÃ©
- Support SSL automatique (Let's Encrypt)
- MÃ©triques Prometheus natives

---

### Couche Ticketing

#### GLPI + MariaDB
```yaml
GLPI:
  Image: diouxx/glpi:latest
  Dependencies: MariaDB
  Health Check: curl -f http://localhost/
  Access: http://glpi.dockerwarts.local

MariaDB:
  Image: mariadb:10.11
  Health Check: mysqladmin ping
  Volumes: glpi-db-data (persistent)
```

**Architecture** :
- GLPI sur frontend network (accessible via Traefik)
- MariaDB sur backend network (isolÃ©e)
- Communication interne sur backend network

---

### Couche Indexation & Recherche

#### Elasticsearch (Cluster)
```yaml
Architecture: Master-Data
Nodes:
  - elasticsearch-master (coordinating + master)
  - elasticsearch-data (data node)

Configuration:
  - Cluster Name: dockerwarts-cluster
  - Discovery: seed_hosts automatique
  - Memory Lock: activÃ©
  - Security: dÃ©sactivÃ©e (interne)

Health Check: curl /_cluster/health
```

**Haute disponibilitÃ©** :
- 2 nÅ“uds minimum pour quorum
- RÃ©plication automatique des shards
- Discovery automatique des nÅ“uds

#### Kibana
```yaml
Role: Interface utilisateur Elasticsearch
Connection: elasticsearch-master:9200
Access: http://kibana.dockerwarts.local
```

---

### Couche Data Lake

#### Cassandra (Cluster)
```yaml
Architecture: Multi-node cluster
Nodes:
  - cassandra-node1 (rack1)
  - cassandra-node2 (rack2)

Configuration:
  - Cluster: DockerwartsCassandraCluster
  - Seeds: cassandra-node1,cassandra-node2
  - Replication: automatique
  - Snitch: GossipingPropertyFileSnitch

Health Check: nodetool status
Start Period: 120s (dÃ©marrage long)
```

**Haute disponibilitÃ©** :
- 2 nÅ“uds avec rÃ©plication
- Gossip protocol pour la dÃ©couverte
- Rack awareness pour distribution

**ScalabilitÃ©** :
- Ajout de nÅ“uds possible sans downtime
- RÃ©plication factor configurable
- Partitionnement automatique

---

### Couche Monitoring

#### Prometheus
```yaml
Role: Time Series Database + Scraper
Scrape Interval: 15s
Targets:
  - prometheus (self)
  - traefik:8080
  - elasticsearch-master:9200
  - elasticsearch-data:9200
  - node-exporter:9100
  - cadvisor:8080

Storage: 30 days retention
```

#### Grafana
```yaml
Role: Visualization & Dashboards
Datasources:
  - Prometheus (default)
  - Elasticsearch

Provisioning:
  - Datasources auto-configurÃ©es
  - Dashboards prÃ©-chargÃ©s

Access: http://grafana.dockerwarts.local
```

#### Node Exporter
```yaml
Role: Host metrics collector
Metrics:
  - CPU usage
  - Memory usage
  - Disk usage
  - Network traffic
```

#### cAdvisor
```yaml
Role: Container metrics collector
Metrics:
  - Container CPU
  - Container Memory
  - Container Network
  - Container Disk I/O
```

---

## Flux de donnÃ©es

### Flux utilisateur (Frontend)

```
User Request
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Traefik â”‚ (Port 80/443)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Route by Host header
     â”œâ”€â”€â”€ grafana.dockerwarts.local â”€â”€â–¶ Grafana:3000
     â”œâ”€â”€â”€ kibana.dockerwarts.local  â”€â”€â–¶ Kibana:5601
     â””â”€â”€â”€ glpi.dockerwarts.local    â”€â”€â–¶ GLPI:80
```

### Flux de monitoring (Backend)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prometheus  â”‚ (Scrape every 15s)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–¶ Traefik:8080/metrics
       â”œâ”€â”€â–¶ Elasticsearch:9200/_prometheus/metrics
       â”œâ”€â”€â–¶ Node Exporter:9100/metrics
       â””â”€â”€â–¶ cAdvisor:8080/metrics
       
       â”‚ Store in TSDB
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grafana    â”‚ Query Prometheus
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de donnÃ©es (Data)

```
Application Data
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Elasticsearch  â”‚ â—€â”€â”€â–¶ Replication
â”‚     Master      â”‚      
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚  Elasticsearch  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚      Data       â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Application Data
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cassandra     â”‚ â—€â”€â”€â–¶ Gossip Protocol
â”‚     Node 1      â”‚      + Data Replication
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      
         â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Cassandra     â”‚
                         â”‚     Node 2      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Gestion de la persistance

### Volumes Docker

| Volume | Service | Type | Backup |
|--------|---------|------|--------|
| `traefik-certs` | Traefik | Certificats SSL | Optionnel |
| `glpi-db-data` | MariaDB | Base de donnÃ©es | **Critique** |
| `glpi-data` | GLPI | Fichiers GLPI | **Critique** |
| `elasticsearch-master-data` | ES Master | Index ES | **Critique** |
| `elasticsearch-data-data` | ES Data | Index ES | **Critique** |
| `kibana-data` | Kibana | Config Kibana | Normal |
| `cassandra-node1-data` | Cassandra 1 | DonnÃ©es | **Critique** |
| `cassandra-node2-data` | Cassandra 2 | DonnÃ©es | **Critique** |
| `grafana-data` | Grafana | Dashboards | Normal |
| `prometheus-data` | Prometheus | MÃ©triques (30j) | Normal |

### StratÃ©gie de sauvegarde

#### Volumes critiques (daily backup)
- GLPI database + data
- Cassandra nodes
- Elasticsearch nodes

#### Volumes normaux (weekly backup)
- Grafana dashboards
- Prometheus data (optionnel, mÃ©triques temporaires)

#### MÃ©thode
```bash
# Automatique via Makefile
make backup

# GÃ©nÃ¨re: backups/dockerwarts_YYYYMMDD_HHMMSS.tar.gz
```

---

## Haute disponibilitÃ©

### StratÃ©gies HA par service

#### 1. Health Checks
Tous les services critiques :
```yaml
healthcheck:
  test: [CMD, ...]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: XYs
```

#### 2. Restart Policies
```yaml
restart: unless-stopped
```

#### 3. Dependencies avec conditions
```yaml
depends_on:
  service-name:
    condition: service_healthy
```

#### 4. Multi-nÅ“uds
- **Elasticsearch** : 2+ nÅ“uds
- **Cassandra** : 2+ nÅ“uds

### Indicateurs de disponibilitÃ©

| Service | SLA Target | HA Mechanism |
|---------|------------|--------------|
| Traefik | 99.9% | Health check + restart |
| Elasticsearch | 99.5% | Cluster 2 nÅ“uds |
| Cassandra | 99.5% | Cluster 2 nÅ“uds |
| Grafana | 99% | Health check + restart |
| GLPI | 99% | Health check + restart + DB rÃ©plication possible |

### Points de dÃ©faillance uniques (SPOF)

âš ï¸ **IdentifiÃ©s** :
- MariaDB (GLPI) : 1 seul nÅ“ud
- Prometheus : 1 seule instance

**Mitigation** :
- Sauvegardes automatiques
- Restart automatique
- Monitoring actif

**Pour production** :
- Ajouter rÃ©plication MariaDB (Master-Slave)
- Ajouter Thanos pour Prometheus HA

---

## Ã‰volution et scalabilitÃ©

### Ajout de nÅ“uds Cassandra

```bash
# Modifier docker-compose.yml
# Ajouter cassandra-node3:
docker-compose up -d cassandra-node3

# Le nÅ“ud rejoint automatiquement le cluster
```

### Ajout de nÅ“uds Elasticsearch

```bash
# Ajouter elasticsearch-data-2 dans docker-compose.yml
docker-compose up -d elasticsearch-data-2
```

### Load Balancing Traefik

Pour production :
- Plusieurs instances Traefik derriÃ¨re un LB (HAProxy, NGINX)
- DNS round-robin
- Keepalived pour IP virtuelle

---

## SÃ©curitÃ©

### ImplÃ©mentÃ©

1. **Isolation rÃ©seau** : Frontend/Backend sÃ©parÃ©s
2. **Firewall applicatif** : Traefik avec rate limiting possible
3. **Secrets** : Variables d'environnement (.env)
4. **Health monitoring** : DÃ©tection rapide des problÃ¨mes

### Recommandations production

1. **SSL/TLS** : Activer Let's Encrypt dans Traefik
2. **Authentification** : 
   - Activer xpack.security dans Elasticsearch
   - OAuth2 pour Grafana
   - LDAP pour GLPI
3. **Secrets management** : Docker Secrets ou Vault
4. **Network policies** : Kubernetes Network Policies
5. **Scanning** : Trivy pour scanner les images

---

## Conclusion

L'architecture DOCKERWARTS offre :

âœ… **Robustesse** : HA sur services critiques
âœ… **ScalabilitÃ©** : Architecture multi-nÅ“uds
âœ… **ObservabilitÃ©** : Monitoring complet
âœ… **SÃ©curitÃ©** : Isolation rÃ©seau + firewall applicatif
âœ… **MaintenabilitÃ©** : Configuration claire et documentÃ©e

**PrÃªt pour la production avec quelques ajustements de sÃ©curitÃ©.**
