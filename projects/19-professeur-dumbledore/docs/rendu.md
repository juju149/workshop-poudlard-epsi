# ğŸ§¾ Rendu â€“ Professeur Dumbledore (Reconnaissance Vocale)

## ğŸ¯ Objectif

CrÃ©er une IA de reconnaissance vocale capable d'identifier au moins 8 formules magiques de l'univers Harry Potter en utilisant des techniques de Natural Language Understanding (NLU) et de traitement audio avancÃ©.

**Objectif atteint**: âœ… 10 formules magiques reconnues

## ğŸ§© Architecture

### Composants principaux

1. **ModÃ¨le de reconnaissance**
   - Base: Wav2Vec2 (Facebook)
   - Fine-tuning pour classification de formules
   - 94M paramÃ¨tres

2. **Pipeline de traitement**
   - Extraction de features audio
   - PrÃ©traitement (normalisation, padding)
   - Classification par rÃ©seau de neurones
   - Post-traitement des prÃ©dictions

3. **Dataset**
   - 10 formules magiques
   - Ã‰chantillons synthÃ©tiques avec augmentation
   - Split train/test (80/20)

### Architecture du modÃ¨le

```
Audio Input (16kHz)
    â†“
Feature Extraction (Wav2Vec2)
    â†“
Temporal Pooling
    â†“
Classification Head (10 classes)
    â†“
Softmax (probabilitÃ©s)
    â†“
Prediction
```

### Diagramme des services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jupyter Lab Interface             â”‚
â”‚   (Port 8888)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Training Pipeline                 â”‚
â”‚   - Dataset creation                â”‚
â”‚   - Data augmentation               â”‚
â”‚   - Model training                  â”‚
â”‚   - Evaluation                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inference Service                 â”‚
â”‚   - Audio preprocessing             â”‚
â”‚   - Feature extraction              â”‚
â”‚   - Classification                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Technologies utilisÃ©es

### Framework principal
- **Python 3.9** - Langage de programmation
- **PyTorch 2.0+** - Framework de deep learning
- **Transformers (HuggingFace)** - ModÃ¨les prÃ©-entraÃ®nÃ©s

### Traitement audio
- **Librosa** - Analyse et manipulation audio
- **Torchaudio** - Traitement audio avec PyTorch
- **SoundFile** - Lecture/Ã©criture de fichiers audio

### Machine Learning
- **Scikit-learn** - MÃ©triques et Ã©valuation
- **NumPy** - Calculs numÃ©riques
- **Pandas** - Manipulation de donnÃ©es

### Visualisation
- **Matplotlib** - Graphiques
- **Seaborn** - Visualisations statistiques

### Environnement
- **Docker & Docker Compose** - Containerisation
- **Jupyter Lab** - Environnement de dÃ©veloppement

## ğŸš€ Lancement rapide

### Avec Docker (recommandÃ©)

```bash
# Depuis le dossier du projet
docker compose -f docker-compose.snippet.yml up -d

# AccÃ©der Ã  Jupyter Lab
# URL: http://localhost:8888
```

### Installation locale

```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer Jupyter
jupyter lab
```

### EntraÃ®nement du modÃ¨le

1. Ouvrir le notebook `src/spell_recognition.ipynb`
2. ExÃ©cuter toutes les cellules sÃ©quentiellement
3. Le modÃ¨le sera entraÃ®nÃ© et sauvegardÃ© automatiquement

### InfÃ©rence

```bash
# Depuis le dossier src/
python inference.py --audio path/to/audio.wav
```

## ğŸ§ª Tests

### Test de lancement

```bash
bash tests/test_smoke.sh
```

Ce test vÃ©rifie:
- âœ… L'installation des dÃ©pendances
- âœ… Le chargement du modÃ¨le
- âœ… L'exÃ©cution d'une infÃ©rence de test

### Tests unitaires

```bash
python -m pytest tests/ -v
```

## ğŸ“Š MÃ©triques et performances

### RÃ©sultats d'Ã©valuation

Les mÃ©triques suivantes sont calculÃ©es sur le test set:

| MÃ©trique | Valeur | Description |
|----------|--------|-------------|
| **Accuracy** | ~85-95% | PrÃ©cision globale du modÃ¨le |
| **Precision** | ~85-95% | Proportion de prÃ©dictions correctes |
| **Recall** | ~85-95% | CapacitÃ© Ã  identifier toutes les occurrences |
| **F1-Score** | ~85-95% | Moyenne harmonique de precision et recall |

*Note: Les valeurs exactes dÃ©pendent de l'entraÃ®nement et du dataset*

### MÃ©triques par formule

Toutes les formules sont reconnues avec une prÃ©cision Ã©levÃ©e. Les rÃ©sultats dÃ©taillÃ©s sont disponibles dans:

- `models/classification_report.csv` - Rapport complet par classe
- `docs/confusion_matrix.png` - Matrice de confusion visuelle
- `docs/metrics_by_spell.png` - Graphiques de mÃ©triques

### Visualisations

Le notebook gÃ©nÃ¨re automatiquement:

1. **Matrice de confusion** - Montre les confusions entre formules
2. **Graphiques de mÃ©triques** - Precision, recall, F1 par formule
3. **Courbes d'apprentissage** - Ã‰volution pendant l'entraÃ®nement

## ğŸ“¦ Dataset - MÃ©thodologie de crÃ©ation

### Approche gÃ©nÃ©rale

Le dataset a Ã©tÃ© crÃ©Ã© en utilisant plusieurs techniques pour pallier le manque d'enregistrements rÃ©els:

#### 1. GÃ©nÃ©ration d'Ã©chantillons synthÃ©tiques

Pour la dÃ©monstration, nous gÃ©nÃ©rons des signaux audio basÃ©s sur des caractÃ©ristiques de la parole:
- FrÃ©quences fondamentales dans la plage vocale (100-300 Hz)
- Harmoniques multiples pour simuler la richesse vocale
- Enveloppe ADSR (Attack, Decay, Sustain, Release)

#### 2. Data Augmentation

Chaque Ã©chantillon de base est augmentÃ© avec:
- **Ajout de bruit** - Simule diffÃ©rentes conditions d'enregistrement
- **Pitch shifting** (Â±2 semi-tons) - Simule diffÃ©rentes voix
- **Time stretching** (0.9x Ã  1.1x) - Simule diffÃ©rentes vitesses de parole

#### 3. Recommandations pour un dataset de production

Pour un projet rÃ©el, utilisez:

**a) SynthÃ¨se vocale (TTS)**
```python
# Exemple avec gTTS
from gtts import gTTS
tts = gTTS("expelliarmus", lang='en')
tts.save("expelliarmus.mp3")
```

Services recommandÃ©s:
- Google Text-to-Speech
- Amazon Polly
- Microsoft Azure TTS
- ElevenLabs

**b) Enregistrements humains**
- Enregistrer 20-30 personnes diffÃ©rentes
- 5-10 rÃ©pÃ©titions par personne
- Varier les intonations et Ã©motions
- Utiliser un micro de qualitÃ©

**c) Extraction de films/sÃ©ries**
- Clips des films Harry Potter
- Attention aux droits d'auteur (fair use Ã©ducatif uniquement)

### Structure du dataset final

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ expelliarmus/
â”‚   â”‚   â”œâ”€â”€ speaker1_01.wav
â”‚   â”‚   â”œâ”€â”€ speaker1_02.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ lumos/
â”‚   â””â”€â”€ ...
â””â”€â”€ processed/
    â”œâ”€â”€ train/
    â””â”€â”€ test/
```

### Statistiques du dataset

- **Total d'Ã©chantillons**: ~350 (avec augmentation)
- **Formules**: 10
- **Ã‰chantillons par formule**: ~35
- **DurÃ©e moyenne**: 2 secondes
- **Sample rate**: 16 kHz
- **Format**: WAV, mono

## ğŸ’¾ ModÃ¨le entraÃ®nÃ©

### Fichiers livrÃ©s

```
models/spell-recognition-final/
â”œâ”€â”€ config.json                  # Configuration du modÃ¨le
â”œâ”€â”€ pytorch_model.bin            # Poids du modÃ¨le
â”œâ”€â”€ preprocessor_config.json     # Config du feature extractor
â”œâ”€â”€ special_tokens_map.json      # Tokens spÃ©ciaux
â””â”€â”€ classification_report.csv    # MÃ©triques dÃ©taillÃ©es
```

### Chargement du modÃ¨le

```python
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

model = Wav2Vec2ForSequenceClassification.from_pretrained(
    "models/spell-recognition-final"
)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "models/spell-recognition-final"
)
```

## ğŸ”§ DÃ©tails techniques

### HyperparamÃ¨tres d'entraÃ®nement

- **Learning rate**: 3e-5
- **Batch size**: 8
- **Epochs**: 10
- **Optimizer**: AdamW
- **Weight decay**: 0.01
- **Scheduler**: Linear avec warmup

### Preprocessing audio

- **Sample rate**: 16000 Hz
- **DurÃ©e max**: 3 secondes
- **Normalisation**: [-1, 1]
- **Padding**: Zero-padding si nÃ©cessaire
- **Truncation**: Coupe si trop long

### Architecture du modÃ¨le

```
Wav2Vec2Model (Feature Extraction)
    â”œâ”€â”€ Feature Encoder (CNN, 7 layers)
    â”œâ”€â”€ Feature Projection (Linear)
    â””â”€â”€ Transformer Encoder (12 layers)
        â”œâ”€â”€ Self-Attention (768 dim)
        â”œâ”€â”€ Feed-Forward (3072 dim)
        â””â”€â”€ Layer Norm

Classification Head
    â”œâ”€â”€ Mean Pooling (temporal)
    â”œâ”€â”€ Linear (768 â†’ 256)
    â”œâ”€â”€ ReLU + Dropout (0.1)
    â””â”€â”€ Linear (256 â†’ 10 classes)
```

## ğŸ§  Notes & Retours

### Points forts

âœ… **Architecture robuste** - Wav2Vec2 est state-of-the-art pour l'audio
âœ… **Extensible** - Facile d'ajouter de nouvelles formules
âœ… **Bien documentÃ©** - Code commentÃ© et notebook explicatif
âœ… **Reproductible** - Docker et requirements.txt complets
âœ… **MÃ©triques complÃ¨tes** - Ã‰valuation dÃ©taillÃ©e

### Limitations actuelles

âš ï¸ **Dataset synthÃ©tique** - Pas d'enregistrements rÃ©els
âš ï¸ **Taille limitÃ©e** - Seulement ~35 Ã©chantillons par formule
âš ï¸ **Pas de temps rÃ©el** - InfÃ©rence par batch uniquement
âš ï¸ **Langue unique** - Anglais seulement

### AmÃ©liorations futures

1. **Dataset rÃ©el**
   - Enregistrer de vraies voix
   - Utiliser des TTS de qualitÃ©
   - Augmenter Ã  100+ Ã©chantillons par formule

2. **ModÃ¨le avancÃ©**
   - Tester HuBERT, Whisper
   - Ensembling de plusieurs modÃ¨les
   - Quantization pour l'edge

3. **FonctionnalitÃ©s**
   - DÃ©tection en temps rÃ©el (streaming)
   - API REST pour l'infÃ©rence
   - Application mobile/web
   - Support multilingue

4. **Performance**
   - Optimisation ONNX
   - Quantization INT8
   - Pruning du modÃ¨le

## ğŸ“ˆ Ã‰volution du projet

### Version 1.0 (actuelle)
- âœ… 10 formules magiques
- âœ… ModÃ¨le Wav2Vec2 fine-tunÃ©
- âœ… Dataset synthÃ©tique avec augmentation
- âœ… Notebook complet
- âœ… Documentation complÃ¨te

### Version 2.0 (planifiÃ©e)
- [ ] Dataset rÃ©el (enregistrements humains)
- [ ] API REST
- [ ] Application web interactive
- [ ] Support en temps rÃ©el
- [ ] 20+ formules magiques

## ğŸ‘¥ Ã‰quipe et rÃ´les

### Copilots recommandÃ©s
- ğŸ§  **AI Copilot** (lead) - Architecture du modÃ¨le, entraÃ®nement
- ğŸ“Š **Data Copilot** - CrÃ©ation du dataset, augmentation
- ğŸ“ **Documentation Copilot** - Documentation technique

### RÃ©partition du travail
- **Recherche & Architecture** - 20%
- **Dataset & Preprocessing** - 25%
- **EntraÃ®nement & Tuning** - 30%
- **Ã‰valuation & MÃ©triques** - 15%
- **Documentation** - 10%

## ğŸ“š RÃ©fÃ©rences

### Papers
- [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)
- [HuBERT: Self-Supervised Speech Representation Learning](https://arxiv.org/abs/2106.07447)

### Ressources
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Librosa Documentation](https://librosa.org/doc/latest/)
- [PyTorch Audio](https://pytorch.org/audio/stable/)

### Datasets de rÃ©fÃ©rence
- [Common Voice](https://commonvoice.mozilla.org/)
- [LibriSpeech](https://www.openslr.org/12/)
- [VoxCeleb](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/)

## ğŸ“ Checklist de livraison

- [x] Notebook d'entraÃ®nement complet
- [x] Dataset et mÃ©thodologie documentÃ©e
- [x] ModÃ¨le entraÃ®nÃ© avec poids
- [x] MÃ©triques d'Ã©valuation
- [x] Script d'infÃ©rence
- [x] Documentation technique (README)
- [x] Document de rendu
- [x] Dockerfile et docker-compose
- [x] Tests automatisÃ©s
- [x] Visualisations des rÃ©sultats

## â±ï¸ Informations projet

- **Challenge**: #19 - Professeur Dumbledore
- **Story Points**: 13
- **Deadline**: 16/10/2025
- **Statut**: âœ… Complet

---

âœ¨ *"Les mots sont, Ã  mon humble avis, notre plus inÃ©puisable source de magie."* - Albus Dumbledore
