{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßô‚Äç‚ôÇÔ∏è IS IT YOU HARRY? - CNN Character Recognition\n",
    "\n",
    "Ce notebook impl√©mente un r√©seau de neurones convolutif (CNN) pour reconna√Ætre **10 personnages** d'Harry Potter.\n",
    "\n",
    "## üìã Objectifs\n",
    "1. Cr√©er/charger un dataset d'images de personnages\n",
    "2. Pr√©traiter les donn√©es (normalisation, augmentation)\n",
    "3. Cr√©er un mod√®le CNN\n",
    "4. Entra√Æner le mod√®le\n",
    "5. √âvaluer les performances (pr√©cision, matrice de confusion)\n",
    "6. Sauvegarder le mod√®le\n",
    "\n",
    "## üë• Personnages reconnus\n",
    "1. Harry Potter\n",
    "2. Hermione Granger\n",
    "3. Ron Weasley\n",
    "4. Albus Dumbledore\n",
    "5. Severus Snape\n",
    "6. Voldemort\n",
    "7. Draco Malfoy\n",
    "8. Hagrid\n",
    "9. Minerva McGonagall\n",
    "10. Sirius Black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\julie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d‚Äôinitialisation d‚Äôune biblioth√®que de liens dynamiques (DLL) a √©chou√©.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d‚Äôinitialisation d‚Äôune biblioth√®que de liens dynamiques (DLL) a √©chou√©.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Deep Learning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[39m\n\u001b[32m     86\u001b[39m     sys.setdlopenflags(_default_dlopen_flags)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback.format_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     92\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     93\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you need help, create an issue \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     94\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mand include the entire stack trace above this error message.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Traceback (most recent call last):\n  File \"C:\\Users\\julie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Une routine d‚Äôinitialisation d‚Äôune biblioth√®que de liens dynamiques (DLL) a √©chou√©.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "BASE_DIR = Path('../data')\n",
    "TRAIN_DIR = BASE_DIR / 'train'\n",
    "VAL_DIR = BASE_DIR / 'val'\n",
    "TEST_DIR = BASE_DIR / 'test'\n",
    "MODEL_DIR = Path('../models')\n",
    "\n",
    "# Hyperparam√®tres\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Personnages (classes)\n",
    "CHARACTERS = [\n",
    "    'harry_potter',\n",
    "    'hermione_granger',\n",
    "    'ron_weasley',\n",
    "    'albus_dumbledore',\n",
    "    'severus_snape',\n",
    "    'voldemort',\n",
    "    'draco_malfoy',\n",
    "    'hagrid',\n",
    "    'minerva_mcgonagall',\n",
    "    'sirius_black'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(CHARACTERS)\n",
    "print(f\"Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {CHARACTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cr√©ation du dataset de d√©monstration\n",
    "\n",
    "‚ö†Ô∏è **Note**: Pour une vraie utilisation, vous devez collecter ~200 images par personnage.\n",
    "\n",
    "Ce code cr√©e un dataset de d√©monstration avec des images synth√©tiques pour tester le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo_dataset():\n",
    "    \"\"\"Cr√©e un dataset de d√©monstration avec des images synth√©tiques.\"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import random\n",
    "    \n",
    "    print(\"üé® Cr√©ation du dataset de d√©monstration...\")\n",
    "    \n",
    "    # Nombre d'images par classe et par split\n",
    "    train_samples = 140  # 70%\n",
    "    val_samples = 30     # 15%\n",
    "    test_samples = 30    # 15%\n",
    "    \n",
    "    for character in tqdm(CHARACTERS, desc=\"Cr√©ation des personnages\"):\n",
    "        # Cr√©er les dossiers\n",
    "        (TRAIN_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        (VAL_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        (TEST_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Cr√©er des images synth√©tiques avec des couleurs diff√©rentes par personnage\n",
    "        char_color = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
    "        \n",
    "        # Images d'entra√Ænement\n",
    "        for i in range(train_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            # Ajouter du bruit pour varier les images\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(TRAIN_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "        \n",
    "        # Images de validation\n",
    "        for i in range(val_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(VAL_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "        \n",
    "        # Images de test\n",
    "        for i in range(test_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(TEST_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "    \n",
    "    print(\"‚úÖ Dataset de d√©monstration cr√©√© avec succ√®s!\")\n",
    "    print(f\"   - Train: {train_samples * NUM_CLASSES} images\")\n",
    "    print(f\"   - Val: {val_samples * NUM_CLASSES} images\")\n",
    "    print(f\"   - Test: {test_samples * NUM_CLASSES} images\")\n",
    "\n",
    "# Cr√©er le dataset si il n'existe pas\n",
    "if not TRAIN_DIR.exists() or len(list(TRAIN_DIR.glob('*/*.jpg'))) == 0:\n",
    "    create_demo_dataset()\n",
    "else:\n",
    "    print(\"‚úÖ Dataset d√©j√† existant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(directory):\n",
    "    \"\"\"Compte les images par classe.\"\"\"\n",
    "    counts = {}\n",
    "    for character in CHARACTERS:\n",
    "        char_dir = directory / character\n",
    "        if char_dir.exists():\n",
    "            counts[character] = len(list(char_dir.glob('*.jpg')))\n",
    "        else:\n",
    "            counts[character] = 0\n",
    "    return counts\n",
    "\n",
    "# Compter les images\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "val_counts = count_images(VAL_DIR)\n",
    "test_counts = count_images(TEST_DIR)\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"üìä Distribution du dataset:\\n\")\n",
    "df_stats = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Validation': val_counts,\n",
    "    'Test': test_counts\n",
    "})\n",
    "df_stats['Total'] = df_stats.sum(axis=1)\n",
    "print(df_stats)\n",
    "print(f\"\\nTotal: {df_stats['Total'].sum()} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser quelques exemples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Exemples d\\'images par personnage', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, character in enumerate(CHARACTERS):\n",
    "    char_dir = TRAIN_DIR / character\n",
    "    images = list(char_dir.glob('*.jpg'))\n",
    "    if images:\n",
    "        img_path = images[0]\n",
    "        img = plt.imread(img_path)\n",
    "        ax = axes[idx // 5, idx % 5]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(character.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Pr√©paration des donn√©es avec Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation pour l'entra√Ænement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Normalisation seulement pour validation et test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# G√©n√©rateurs de donn√©es\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nClasses d√©tect√©es: {train_generator.class_indices}\")\n",
    "print(f\"Nombre d'images d'entra√Ænement: {train_generator.samples}\")\n",
    "print(f\"Nombre d'images de validation: {val_generator.samples}\")\n",
    "print(f\"Nombre d'images de test: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Architecture du mod√®le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le CNN simple pour la classification d'images.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 blocs convolutifs avec pooling\n",
    "    - Couches fully connected\n",
    "    - Dropout pour r√©gularisation\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bloc 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Sortie\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = create_cnn_model(input_shape=(*IMG_SIZE, 3), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"\\nüèóÔ∏è Architecture du mod√®le:\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le dossier pour les mod√®les\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        MODEL_DIR / 'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nüéì D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualisation des courbes d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Pr√©cision\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Pr√©cision du mod√®le', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Pr√©cision')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perte\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Perte du mod√®le', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Perte')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Graphiques sauvegard√©s dans {MODEL_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ √âvaluation sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le\n",
    "model = keras.models.load_model(MODEL_DIR / 'best_model.h5')\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\nüß™ √âvaluation sur le test set...\\n\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats finaux:\")\n",
    "print(f\"   - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   - Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[c.replace('_', ' ').title() for c in CHARACTERS],\n",
    "    yticklabels=[c.replace('_', ' ').title() for c in CHARACTERS],\n",
    "    cbar_kws={'label': 'Nombre de pr√©dictions'}\n",
    ")\n",
    "plt.title('Matrice de Confusion - Reconnaissance de Personnages', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Pr√©diction', fontsize=12)\n",
    "plt.ylabel('V√©rit√©', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Matrice de confusion sauvegard√©e dans {MODEL_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Rapport de classification d√©taill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "class_names = [c.replace('_', ' ').title() for c in CHARACTERS]\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "print(\"\\nüìù Rapport de classification:\\n\")\n",
    "print(df_report.round(3))\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "df_report.to_csv(MODEL_DIR / 'classification_report.csv')\n",
    "print(f\"\\nüìä Rapport sauvegard√© dans {MODEL_DIR / 'classification_report.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde du mod√®le final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = MODEL_DIR / 'character_recognition_final.h5'\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úÖ Mod√®le final sauvegard√© dans {final_model_path}\")\n",
    "\n",
    "# Sauvegarder les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'model_name': 'Harry Potter Character Recognition CNN',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'characters': CHARACTERS,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(MODEL_DIR / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ M√©tadonn√©es sauvegard√©es dans {MODEL_DIR / 'model_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Analyse des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les erreurs de classification\n",
    "errors = np.where(y_pred != y_true)[0]\n",
    "print(f\"\\n‚ùå Nombre d'erreurs: {len(errors)} / {len(y_true)} ({len(errors)/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "if len(errors) > 0:\n",
    "    # Analyser les erreurs les plus fr√©quentes\n",
    "    error_pairs = []\n",
    "    for idx in errors:\n",
    "        true_class = CHARACTERS[y_true[idx]]\n",
    "        pred_class = CHARACTERS[y_pred[idx]]\n",
    "        error_pairs.append((true_class, pred_class))\n",
    "    \n",
    "    # Compter les paires d'erreurs\n",
    "    from collections import Counter\n",
    "    error_counts = Counter(error_pairs)\n",
    "    \n",
    "    print(\"\\nüîç Erreurs les plus fr√©quentes:\")\n",
    "    for (true_class, pred_class), count in error_counts.most_common(10):\n",
    "        print(f\"   {true_class} ‚Üí {pred_class}: {count} fois\")\n",
    "else:\n",
    "    print(\"\\nüéâ Aucune erreur! Mod√®le parfait!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ R√âSUM√â FINAL - IS IT YOU HARRY?\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s!\")\n",
    "print(f\"\\nüìä Statistiques:\")\n",
    "print(f\"   - Nombre de personnages: {NUM_CLASSES}\")\n",
    "print(f\"   - Images d'entra√Ænement: {train_generator.samples}\")\n",
    "print(f\"   - Images de validation: {val_generator.samples}\")\n",
    "print(f\"   - Images de test: {test_generator.samples}\")\n",
    "print(f\"   - Taille des images: {IMG_SIZE}\")\n",
    "print(f\"\\nüéØ Performances:\")\n",
    "print(f\"   - Pr√©cision sur test: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"   - Perte sur test: {test_loss:.4f}\")\n",
    "print(f\"   - Epochs entra√Æn√©s: {len(history.history['loss'])}\")\n",
    "print(f\"\\nüíæ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   - Mod√®le: {final_model_path}\")\n",
    "print(f\"   - Courbes d'entra√Ænement: {MODEL_DIR / 'training_curves.png'}\")\n",
    "print(f\"   - Matrice de confusion: {MODEL_DIR / 'confusion_matrix.png'}\")\n",
    "print(f\"   - Rapport de classification: {MODEL_DIR / 'classification_report.csv'}\")\n",
    "print(f\"   - M√©tadonn√©es: {MODEL_DIR / 'model_metadata.json'}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Conclusion\n",
    "if test_accuracy >= 0.9:\n",
    "    print(\"\\nüåü Excellent! Le mod√®le atteint une pr√©cision sup√©rieure √† 90%!\")\n",
    "elif test_accuracy >= 0.8:\n",
    "    print(\"\\n‚ú® Tr√®s bien! Le mod√®le atteint une pr√©cision sup√©rieure √† 80%!\")\n",
    "elif test_accuracy >= 0.7:\n",
    "    print(\"\\nüëç Bien! Le mod√®le atteint une pr√©cision sup√©rieure √† 70%!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Le mod√®le pourrait √™tre am√©lior√©. Consid√©rez:\")\n",
    "    print(\"   - Plus de donn√©es d'entra√Ænement\")\n",
    "    print(\"   - Data augmentation plus aggressive\")\n",
    "    print(\"   - Architecture plus profonde\")\n",
    "    print(\"   - Transfer learning (ex: VGG16, ResNet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TensorFlow kernel diagnostic ---\n",
    "import sys, os, traceback, importlib.util\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "print('\\n=== TensorFlow kernel diagnostic ===\\n')\n",
    "print('Python executable:')\n",
    "print(sys.executable)\n",
    "print('\\nPython version:')\n",
    "print(sys.version)\n",
    "print('\\nArchitecture:')\n",
    "print(platform.architecture())\n",
    "\n",
    "print('\\n--- first 12 entries of sys.path ---')\n",
    "for p in sys.path[:12]:\n",
    "    print(p)\n",
    "\n",
    "print('\\n--- PATH (first 24 entries) ---')\n",
    "path_entries = os.environ.get('PATH','').split(os.pathsep)\n",
    "for p in path_entries[:24]:\n",
    "    print(p)\n",
    "\n",
    "print('\\n--- Look for MSVC runtime DLLs in PATH/System dirs ---')\n",
    "dll_names = ['vcruntime140_1.dll','vcruntime140.dll','msvcp140.dll']\n",
    "sys_dirs = [r'C:\\Windows\\System32', r'C:\\Windows\\SysWOW64']\n",
    "found = {}\n",
    "for dll in dll_names:\n",
    "    found[dll] = []\n",
    "    for d in path_entries:\n",
    "        try:\n",
    "            p = Path(d) / dll\n",
    "            if p.exists():\n",
    "                found[dll].append(str(p))\n",
    "        except Exception:\n",
    "            pass\n",
    "    for d in sys_dirs:\n",
    "        p = Path(d) / dll\n",
    "        if p.exists():\n",
    "            found[dll].append(str(p))\n",
    "\n",
    "for dll, locs in found.items():\n",
    "    print(f\"{dll}: {'FOUND' if locs else 'NOT FOUND'}\")\n",
    "    if locs:\n",
    "        for l in locs:\n",
    "            print('  -', l)\n",
    "\n",
    "print('\\n--- tensorflow package discovery ---')\n",
    "spec = importlib.util.find_spec('tensorflow')\n",
    "if spec is None:\n",
    "    print('tensorflow not found via importlib.util.find_spec')\n",
    "else:\n",
    "    print('tensorflow spec found, origin:', spec.origin)\n",
    "\n",
    "print('\\n--- Try importing tensorflow (full traceback on failure) ---')\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print('Imported tensorflow successfully')\n",
    "    print('tf.__version__ =', tf.__version__)\n",
    "    print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "\n",
    "print('\\n=== Diagnostic finished ===')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
