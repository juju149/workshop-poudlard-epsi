{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßô‚Äç‚ôÇÔ∏è IS IT YOU HARRY? - CNN Character Recognition\n",
    "\n",
    "Ce notebook impl√©mente un r√©seau de neurones convolutif (CNN) pour reconna√Ætre **10 personnages** d'Harry Potter.\n",
    "\n",
    "## üìã Objectifs\n",
    "1. Cr√©er/charger un dataset d'images de personnages\n",
    "2. Pr√©traiter les donn√©es (normalisation, augmentation)\n",
    "3. Cr√©er un mod√®le CNN\n",
    "4. Entra√Æner le mod√®le\n",
    "5. √âvaluer les performances (pr√©cision, matrice de confusion)\n",
    "6. Sauvegarder le mod√®le\n",
    "\n",
    "## üë• Personnages reconnus\n",
    "1. Harry Potter\n",
    "2. Hermione Granger\n",
    "3. Ron Weasley\n",
    "4. Albus Dumbledore\n",
    "5. Severus Snape\n",
    "6. Voldemort\n",
    "7. Draco Malfoy\n",
    "8. Hagrid\n",
    "9. Minerva McGonagall\n",
    "10. Sirius Black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Import des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins\n",
    "BASE_DIR = Path('../data')\n",
    "TRAIN_DIR = BASE_DIR / 'train'\n",
    "VAL_DIR = BASE_DIR / 'val'\n",
    "TEST_DIR = BASE_DIR / 'test'\n",
    "MODEL_DIR = Path('../models')\n",
    "\n",
    "# Hyperparam√®tres\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Personnages (classes)\n",
    "CHARACTERS = [\n",
    "    'harry_potter',\n",
    "    'hermione_granger',\n",
    "    'ron_weasley',\n",
    "    'albus_dumbledore',\n",
    "    'severus_snape',\n",
    "    'voldemort',\n",
    "    'draco_malfoy',\n",
    "    'hagrid',\n",
    "    'minerva_mcgonagall',\n",
    "    'sirius_black'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(CHARACTERS)\n",
    "print(f\"Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {CHARACTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cr√©ation du dataset de d√©monstration\n",
    "\n",
    "‚ö†Ô∏è **Note**: Pour une vraie utilisation, vous devez collecter ~200 images par personnage.\n",
    "\n",
    "Ce code cr√©e un dataset de d√©monstration avec des images synth√©tiques pour tester le pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo_dataset():\n",
    "    \"\"\"Cr√©e un dataset de d√©monstration avec des images synth√©tiques.\"\"\"\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import random\n",
    "    \n",
    "    print(\"üé® Cr√©ation du dataset de d√©monstration...\")\n",
    "    \n",
    "    # Nombre d'images par classe et par split\n",
    "    train_samples = 140  # 70%\n",
    "    val_samples = 30     # 15%\n",
    "    test_samples = 30    # 15%\n",
    "    \n",
    "    for character in tqdm(CHARACTERS, desc=\"Cr√©ation des personnages\"):\n",
    "        # Cr√©er les dossiers\n",
    "        (TRAIN_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        (VAL_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        (TEST_DIR / character).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Cr√©er des images synth√©tiques avec des couleurs diff√©rentes par personnage\n",
    "        char_color = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
    "        \n",
    "        # Images d'entra√Ænement\n",
    "        for i in range(train_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            # Ajouter du bruit pour varier les images\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(TRAIN_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "        \n",
    "        # Images de validation\n",
    "        for i in range(val_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(VAL_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "        \n",
    "        # Images de test\n",
    "        for i in range(test_samples):\n",
    "            img = Image.new('RGB', IMG_SIZE, color=char_color)\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            for _ in range(100):\n",
    "                x, y = random.randint(0, IMG_SIZE[0]-1), random.randint(0, IMG_SIZE[1]-1)\n",
    "                noise_color = tuple(min(255, max(0, c + random.randint(-50, 50))) for c in char_color)\n",
    "                draw.point((x, y), fill=noise_color)\n",
    "            img.save(TEST_DIR / character / f\"{character}_{i:04d}.jpg\")\n",
    "    \n",
    "    print(\"‚úÖ Dataset de d√©monstration cr√©√© avec succ√®s!\")\n",
    "    print(f\"   - Train: {train_samples * NUM_CLASSES} images\")\n",
    "    print(f\"   - Val: {val_samples * NUM_CLASSES} images\")\n",
    "    print(f\"   - Test: {test_samples * NUM_CLASSES} images\")\n",
    "\n",
    "# Cr√©er le dataset si il n'existe pas\n",
    "if not TRAIN_DIR.exists() or len(list(TRAIN_DIR.glob('*/*.jpg'))) == 0:\n",
    "    create_demo_dataset()\n",
    "else:\n",
    "    print(\"‚úÖ Dataset d√©j√† existant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploration du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(directory):\n",
    "    \"\"\"Compte les images par classe.\"\"\"\n",
    "    counts = {}\n",
    "    for character in CHARACTERS:\n",
    "        char_dir = directory / character\n",
    "        if char_dir.exists():\n",
    "            counts[character] = len(list(char_dir.glob('*.jpg')))\n",
    "        else:\n",
    "            counts[character] = 0\n",
    "    return counts\n",
    "\n",
    "# Compter les images\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "val_counts = count_images(VAL_DIR)\n",
    "test_counts = count_images(TEST_DIR)\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"üìä Distribution du dataset:\\n\")\n",
    "df_stats = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Validation': val_counts,\n",
    "    'Test': test_counts\n",
    "})\n",
    "df_stats['Total'] = df_stats.sum(axis=1)\n",
    "print(df_stats)\n",
    "print(f\"\\nTotal: {df_stats['Total'].sum()} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser quelques exemples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Exemples d\\'images par personnage', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, character in enumerate(CHARACTERS):\n",
    "    char_dir = TRAIN_DIR / character\n",
    "    images = list(char_dir.glob('*.jpg'))\n",
    "    if images:\n",
    "        img_path = images[0]\n",
    "        img = plt.imread(img_path)\n",
    "        ax = axes[idx // 5, idx % 5]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(character.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Pr√©paration des donn√©es avec Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation pour l'entra√Ænement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Normalisation seulement pour validation et test\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# G√©n√©rateurs de donn√©es\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nClasses d√©tect√©es: {train_generator.class_indices}\")\n",
    "print(f\"Nombre d'images d'entra√Ænement: {train_generator.samples}\")\n",
    "print(f\"Nombre d'images de validation: {val_generator.samples}\")\n",
    "print(f\"Nombre d'images de test: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Architecture du mod√®le CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape=(128, 128, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le CNN simple pour la classification d'images.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 blocs convolutifs avec pooling\n",
    "    - Couches fully connected\n",
    "    - Dropout pour r√©gularisation\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Bloc 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Bloc 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Sortie\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "model = create_cnn_model(input_shape=(*IMG_SIZE, 3), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"\\nüèóÔ∏è Architecture du mod√®le:\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Entra√Ænement du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le dossier pour les mod√®les\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        MODEL_DIR / 'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nüéì D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualisation des courbes d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Pr√©cision\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Pr√©cision du mod√®le', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Pr√©cision')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perte\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Perte du mod√®le', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Perte')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Graphiques sauvegard√©s dans {MODEL_DIR / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ √âvaluation sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le\n",
    "model = keras.models.load_model(MODEL_DIR / 'best_model.h5')\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\nüß™ √âvaluation sur le test set...\\n\")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nüìä R√©sultats finaux:\")\n",
    "print(f\"   - Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   - Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[c.replace('_', ' ').title() for c in CHARACTERS],\n",
    "    yticklabels=[c.replace('_', ' ').title() for c in CHARACTERS],\n",
    "    cbar_kws={'label': 'Nombre de pr√©dictions'}\n",
    ")\n",
    "plt.title('Matrice de Confusion - Reconnaissance de Personnages', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Pr√©diction', fontsize=12)\n",
    "plt.ylabel('V√©rit√©', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Matrice de confusion sauvegard√©e dans {MODEL_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Rapport de classification d√©taill√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "class_names = [c.replace('_', ' ').title() for c in CHARACTERS]\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=class_names,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "print(\"\\nüìù Rapport de classification:\\n\")\n",
    "print(df_report.round(3))\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "df_report.to_csv(MODEL_DIR / 'classification_report.csv')\n",
    "print(f\"\\nüìä Rapport sauvegard√© dans {MODEL_DIR / 'classification_report.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde du mod√®le final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = MODEL_DIR / 'character_recognition_final.h5'\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úÖ Mod√®le final sauvegard√© dans {final_model_path}\")\n",
    "\n",
    "# Sauvegarder les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'model_name': 'Harry Potter Character Recognition CNN',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'characters': CHARACTERS,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(MODEL_DIR / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ M√©tadonn√©es sauvegard√©es dans {MODEL_DIR / 'model_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Analyse des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les erreurs de classification\n",
    "errors = np.where(y_pred != y_true)[0]\n",
    "print(f\"\\n‚ùå Nombre d'erreurs: {len(errors)} / {len(y_true)} ({len(errors)/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "if len(errors) > 0:\n",
    "    # Analyser les erreurs les plus fr√©quentes\n",
    "    error_pairs = []\n",
    "    for idx in errors:\n",
    "        true_class = CHARACTERS[y_true[idx]]\n",
    "        pred_class = CHARACTERS[y_pred[idx]]\n",
    "        error_pairs.append((true_class, pred_class))\n",
    "    \n",
    "    # Compter les paires d'erreurs\n",
    "    from collections import Counter\n",
    "    error_counts = Counter(error_pairs)\n",
    "    \n",
    "    print(\"\\nüîç Erreurs les plus fr√©quentes:\")\n",
    "    for (true_class, pred_class), count in error_counts.most_common(10):\n",
    "        print(f\"   {true_class} ‚Üí {pred_class}: {count} fois\")\n",
    "else:\n",
    "    print(\"\\nüéâ Aucune erreur! Mod√®le parfait!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ R√âSUM√â FINAL - IS IT YOU HARRY?\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s!\")\n",
    "print(f\"\\nüìä Statistiques:\")\n",
    "print(f\"   - Nombre de personnages: {NUM_CLASSES}\")\n",
    "print(f\"   - Images d'entra√Ænement: {train_generator.samples}\")\n",
    "print(f\"   - Images de validation: {val_generator.samples}\")\n",
    "print(f\"   - Images de test: {test_generator.samples}\")\n",
    "print(f\"   - Taille des images: {IMG_SIZE}\")\n",
    "print(f\"\\nüéØ Performances:\")\n",
    "print(f\"   - Pr√©cision sur test: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"   - Perte sur test: {test_loss:.4f}\")\n",
    "print(f\"   - Epochs entra√Æn√©s: {len(history.history['loss'])}\")\n",
    "print(f\"\\nüíæ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   - Mod√®le: {final_model_path}\")\n",
    "print(f\"   - Courbes d'entra√Ænement: {MODEL_DIR / 'training_curves.png'}\")\n",
    "print(f\"   - Matrice de confusion: {MODEL_DIR / 'confusion_matrix.png'}\")\n",
    "print(f\"   - Rapport de classification: {MODEL_DIR / 'classification_report.csv'}\")\n",
    "print(f\"   - M√©tadonn√©es: {MODEL_DIR / 'model_metadata.json'}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Conclusion\n",
    "if test_accuracy >= 0.9:\n",
    "    print(\"\\nüåü Excellent! Le mod√®le atteint une pr√©cision sup√©rieure √† 90%!\")\n",
    "elif test_accuracy >= 0.8:\n",
    "    print(\"\\n‚ú® Tr√®s bien! Le mod√®le atteint une pr√©cision sup√©rieure √† 80%!\")\n",
    "elif test_accuracy >= 0.7:\n",
    "    print(\"\\nüëç Bien! Le mod√®le atteint une pr√©cision sup√©rieure √† 70%!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Le mod√®le pourrait √™tre am√©lior√©. Consid√©rez:\")\n",
    "    print(\"   - Plus de donn√©es d'entra√Ænement\")\n",
    "    print(\"   - Data augmentation plus aggressive\")\n",
    "    print(\"   - Architecture plus profonde\")\n",
    "    print(\"   - Transfer learning (ex: VGG16, ResNet)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
