# Notes de m√©thodologie - Nimbus 3000

## üìã Vue d'ensemble

Ce document contient des notes sur la m√©thodologie, les choix de conception, et les meilleures pratiques pour le benchmark d'optimizers.

## üéØ Objectifs du benchmark

1. **Comparer √©quitablement** 7 optimizers sur la m√™me t√¢che
2. **Mesurer statistiquement** la performance avec plusieurs seeds
3. **Analyser** les trade-offs vitesse/pr√©cision
4. **Documenter** de mani√®re reproductible

## üî¨ Protocole exp√©rimental

### Pourquoi ces choix ?

**Epochs: 50**
- Suffisant pour la convergence sur ce dataset
- Early stopping √©vite le surapprentissage
- Compromis entre temps de calcul et r√©sultats

**Batch size: 64**
- Standard pour CNNs de cette taille
- Compromis m√©moire GPU / stabilit√© training
- Peut √™tre r√©duit √† 32 si probl√®me de m√©moire

**Seeds: 5 diff√©rents**
- Minimum pour statistiques robustes
- Permet de calculer moyenne ¬± √©cart-type
- R√©v√®le la stabilit√© des optimizers

**Learning rate scheduler: Cosine Annealing**
- Am√©liore la convergence finale
- R√©duit le LR progressivement
- Standard dans la litt√©rature

**Gradient clipping: 1.0**
- √âvite les explosions de gradients
- Particuli√®rement utile pour SGD
- Valeur standard

### M√©triques choisies

**Accuracy**
- M√©trique principale, facile √† interpr√©ter
- Suffit pour dataset √©quilibr√©

**F1-score (macro)**
- Sensible aux d√©s√©quilibres de classes
- Plus robuste que accuracy seule
- Standard pour classification multiclasse

**Loss (CrossEntropy)**
- Mesure directe de l'optimisation
- Indicateur de calibration du mod√®le

**Temps d'entra√Ænement**
- Critique pour applications r√©elles
- R√©v√®le l'efficacit√© computationnelle

## üß™ Grilles d'hyperparam√®tres

### Philosophie

- **4 configs par optimizer**: compromis entre exploration et temps
- **Focus sur learning rate**: param√®tre le plus impactant
- **Variations sur param√®tres sp√©cifiques**: momentum pour SGD, betas pour Adam, etc.

### SGD

```
Config 0: lr=0.1, momentum=0.0           # Baseline sans momentum
Config 1: lr=0.1, momentum=0.9           # Standard avec momentum
Config 2: lr=0.1, momentum=0.9, nesterov # Nesterov acceleration
Config 3: lr=0.01, momentum=0.9          # LR plus conservateur
```

**Rationale**: 
- SGD n√©cessite g√©n√©ralement LR plus √©lev√©
- Momentum aide beaucoup √† la convergence
- Nesterov peut am√©liorer l√©g√®rement

### Adam / AdamW

```
Config 0: lr=1e-3, standard betas        # Configuration classique
Config 1: lr=3e-4, standard betas        # LR recommand√© par beaucoup
Config 2: lr=1e-3, beta2=0.95            # Beta2 plus bas (moins de smoothing)
Config 3: lr=3e-4, beta2=0.95            # Combinaison conservatrice
```

**Rationale**:
- 1e-3 est le d√©faut Adam mais peut √™tre trop √©lev√©
- 3e-4 souvent meilleur en pratique
- Beta2 = 0.95 peut aider pour datasets bruit√©s

### RMSProp

```
Config 0: lr=1e-3, alpha=0.9             # Standard
Config 1: lr=3e-4, alpha=0.9             # LR plus bas
Config 2: lr=1e-3, alpha=0.95, centered  # Plus de smoothing + centered
Config 3: lr=3e-4, alpha=0.95, centered  # Version conservatrice
```

**Rationale**:
- Centered RMSProp peut am√©liorer la stabilit√©
- Alpha contr√¥le le smoothing (plus haut = plus smooth)

## üìä Interpr√©tation des r√©sultats

### Ce qu'on cherche

**Meilleur optimizer pour accuracy**
- Qui atteint le plus haut score test ?
- Trade-off avec le temps ?

**Optimizer le plus stable**
- Qui a le plus faible √©cart-type entre seeds ?
- Important pour reproductibilit√©

**Optimizer le plus rapide**
- Temps total pour atteindre convergence
- Nombre d'epochs n√©cessaires

**Meilleur trade-off**
- Balance entre tous les crit√®res
- Recommandation g√©n√©rale

### Analyse statistique

**Moyenne ¬± √âcart-type**
- √âcart-type < 1% : tr√®s stable
- √âcart-type 1-3% : normal
- √âcart-type > 5% : instable

**Comparaisons**
- Diff√©rence > 2% : probablement significative
- Diff√©rence < 0.5% : probablement n√©gligeable
- Tests statistiques (t-test) pour confirmer

## üé® Visualisations

### Courbes d'entra√Ænement

**Par optimizer**
- Montrer toutes les runs (transparence)
- Identifier convergence et stabilit√©
- D√©tecter overfitting

**Comparaison globale**
- Meilleure config par optimizer
- Facile de voir qui converge vite
- Qui atteint le meilleur score

### Graphiques de comparaison

**Barres d'accuracy**
- Avec barres d'erreur (¬±std)
- Tri√© par performance
- Couleurs distinctes

**Scatter accuracy vs temps**
- Trade-off visualis√©
- Pareto frontier
- Guide le choix selon contraintes

**Heatmap**
- Toutes les configs visualis√©es
- Patterns dans hyperparams
- Sensibilit√© r√©v√©l√©e

## üêõ Probl√®mes courants

### CUDA Out of Memory

**Solutions**:
1. R√©duire batch size: `--batch-size 32`
2. R√©duire input size: `--input-size 96`
3. Utiliser gradient accumulation
4. Tester avec CPU d'abord: `CUDA_VISIBLE_DEVICES="" python ...`

### Convergence lente

**Causes possibles**:
- Learning rate trop bas ou trop haut
- Data augmentation trop agressive
- Architecture mal initialis√©e

**Debug**:
- Visualiser les courbes de loss
- V√©rifier les gradients (pas NaN)
- Tester sans data augmentation

### Variance √©lev√©e entre seeds

**Normal si**:
- Dataset petit
- Model complexe
- Task difficile

**Anormal si**:
- Variance > 10%
- Patterns erratiques
- Peut indiquer bug

## üìù Bonnes pratiques

### Avant de lancer le benchmark complet

1. **Smoke test**: V√©rifier que tout marche
2. **Test 1 epoch**: V√©rifier temps estim√©
3. **Test 1 optimizer complet**: V√©rifier workflow
4. **Puis lancer full grid**

### Pendant l'ex√©cution

1. **Monitorer**: V√©rifier logs r√©guli√®rement
2. **Checkpoints**: S'assurer qu'ils sont sauv√©s
3. **Disk space**: V√©rifier l'espace disque
4. **Tuer jobs bloqu√©s**: Si timeout

### Apr√®s l'ex√©cution

1. **V√©rifier tous les runs**: Pas d'erreurs
2. **Agr√©ger imm√©diatement**: Avant de perdre les logs
3. **Backup**: Copier runs/ ailleurs
4. **Documenter**: Notes sur anomalies

## üéì Pour le rapport acad√©mique

### Structure √† respecter

1. **R√©sum√©** (150-200 mots)
   - Contexte, m√©thode, r√©sultats cl√©s
   
2. **Introduction** (1-2 pages)
   - Motivation
   - Contributions
   - Plan du papier

3. **Related Work** (0.5-1 page)
   - Autres benchmarks d'optimizers
   - Citer papiers originaux

4. **M√©thodologie** (2-3 pages)
   - Dataset, mod√®le, protocole
   - Justifier les choix
   - Reproductibilit√©

5. **R√©sultats** (2-3 pages)
   - Tables, figures
   - Observations factuelles
   - Pas d'interpr√©tation encore

6. **Discussion** (1-2 pages)
   - Interpr√©tation
   - Comparaison litt√©rature
   - Recommandations

7. **Conclusion** (0.5-1 page)
   - R√©sum√© contributions
   - Limitations
   - Future work

### Figures de qualit√©

- **DPI minimum**: 300 pour publication
- **Labels**: Lisibles, taille police >= 10
- **L√©gendes**: Compl√®tes et informatives
- **Couleurs**: Colorblind-friendly
- **Captions**: D√©crivent ce qu'on voit

### Tables

- **Alignement**: Nombres align√©s √† droite
- **Pr√©cision**: 4 d√©cimales pour m√©triques
- **Bold**: Meilleurs r√©sultats en gras
- **L√©gende**: Explique abbr√©viations

## üöÄ Extensions possibles

### Court terme

1. **Plus de seeds**: 10 au lieu de 5
2. **Plus d'epochs**: 100 au lieu de 50
3. **Cross-validation**: K-fold
4. **Ensemble**: Moyenne de plusieurs runs

### Moyen terme

1. **Autres architectures**: ResNet, ViT
2. **Autres datasets**: CIFAR, ImageNet
3. **Transfer learning**: Fine-tuning
4. **Mixed precision**: AMP

### Long terme

1. **AutoML**: Optimisation automatique hyperparams
2. **Neural Architecture Search**: Chercher architecture
3. **Multi-task**: Plusieurs t√¢ches ensemble
4. **Distributed**: Training multi-GPU

## üìö R√©f√©rences utiles

### Papers

- Adam: Kingma & Ba (2014)
- AdamW: Loshchilov & Hutter (2017)
- Adan: Xie et al. (2022)
- SGD momentum: Sutskever et al. (2013)

### Ressources

- PyTorch optimizers doc
- Papers With Code (benchmarks)
- Sebastian Ruder's optimization overview
- Distill.pub (visualizations)

## ‚ú® Conseils finaux

1. **Patience**: Benchmark prend du temps
2. **Rigueur**: Suivre protocole strictement
3. **Documentation**: Noter tout
4. **Backup**: Sauvegarder r√©sultats
5. **Visualisation**: Explorer les donn√©es
6. **Interpr√©tation**: R√©fl√©chir aux r√©sultats
7. **Communication**: Rapport clair

Bon benchmark ! üßô‚Äç‚ôÇÔ∏è‚ú®
