{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Notebook 1 - Ingestion et Nettoyage des Textes Harry Potter\n",
    "\n",
    "Ce notebook extrait et nettoie les textes des livres Harry Potter depuis les PDFs.\n",
    "\n",
    "## Objectifs\n",
    "1. Extraire le texte brut des 7 livres PDF\n",
    "2. Nettoyer et normaliser le texte\n",
    "3. Segmenter par phrases et dialogues\n",
    "4. Exporter en format parquet pour analyse ult√©rieure\n",
    "\n",
    "## Sorties\n",
    "- `data/sentences.parquet` : corpus segment√© par phrases\n",
    "- `data/book_metadata.json` : m√©tadonn√©es des livres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration des chemins\n",
    "NOTEBOOK_DIR = Path().absolute()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "BOOKS_DIR = PROJECT_ROOT / \"../../context/books\"\n",
    "DATA_DIR = NOTEBOOK_DIR.parent / \"data\"\n",
    "\n",
    "# Cr√©er le dossier data si n√©cessaire\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìö Books directory: {BOOKS_DIR}\")\n",
    "print(f\"üíæ Data output: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©tadonn√©es des livres\n",
    "BOOK_INFO = {\n",
    "    \"harry-potter-1-lecole-des-sorciers.pdf\": {\n",
    "        \"title\": \"L'√âcole des Sorciers\",\n",
    "        \"book_number\": 1,\n",
    "        \"pages\": 320,\n",
    "        \"year\": 1997\n",
    "    },\n",
    "    \"harry-potter-2-la-chambre-des-secrets.pdf\": {\n",
    "        \"title\": \"La Chambre des Secrets\",\n",
    "        \"book_number\": 2,\n",
    "        \"pages\": 360,\n",
    "        \"year\": 1998\n",
    "    },\n",
    "    \"harry-potter-3-le-prisonnier-dazkaban.pdf\": {\n",
    "        \"title\": \"Le Prisonnier d'Azkaban\",\n",
    "        \"book_number\": 3,\n",
    "        \"pages\": 420,\n",
    "        \"year\": 1999\n",
    "    },\n",
    "    \"harry-potter-4-la-coupe-de-feu.pdf\": {\n",
    "        \"title\": \"La Coupe de Feu\",\n",
    "        \"book_number\": 4,\n",
    "        \"pages\": 656,\n",
    "        \"year\": 2000\n",
    "    },\n",
    "    \"harry-potter-5-lordre-du-phoenix.pdf\": {\n",
    "        \"title\": \"L'Ordre du Ph√©nix\",\n",
    "        \"book_number\": 5,\n",
    "        \"pages\": 980,\n",
    "        \"year\": 2003\n",
    "    },\n",
    "    \"harry-potter-6-le-prince-de-sang-mecc82lecc81.pdf\": {\n",
    "        \"title\": \"Le Prince de Sang-M√™l√©\",\n",
    "        \"book_number\": 6,\n",
    "        \"pages\": 640,\n",
    "        \"year\": 2005\n",
    "    },\n",
    "    \"harry-potter-7-les-reliques-de-la-mort.pdf\": {\n",
    "        \"title\": \"Les Reliques de la Mort\",\n",
    "        \"book_number\": 7,\n",
    "        \"pages\": 800,\n",
    "        \"year\": 2007\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìö Nombre de livres configur√©s: {len(BOOK_INFO)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction du texte depuis les PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"  üìÑ Pages: {len(pdf_reader.pages)}\")\n",
    "            \n",
    "            for page_num, page in enumerate(tqdm(pdf_reader.pages, desc=\"Extraction\")):\n",
    "                try:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Erreur page {page_num}: {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erreur lecture PDF: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte extrait.\"\"\"\n",
    "    # Supprimer les sauts de ligne excessifs\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Normaliser les guillemets\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201c', '¬´').replace('\\u201d', '¬ª')\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire le texte de tous les livres\n",
    "books_data = []\n",
    "\n",
    "for filename, info in BOOK_INFO.items():\n",
    "    pdf_path = BOOKS_DIR / filename\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Livre non trouv√©: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìñ Extraction: {info['title']}\")\n",
    "    \n",
    "    # Extraire et nettoyer\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    clean = clean_text(raw_text)\n",
    "    \n",
    "    # Statistiques de base\n",
    "    word_count = len(clean.split())\n",
    "    char_count = len(clean)\n",
    "    \n",
    "    print(f\"  ‚úÖ Mots: {word_count:,}\")\n",
    "    print(f\"  ‚úÖ Caract√®res: {char_count:,}\")\n",
    "    \n",
    "    books_data.append({\n",
    "        'filename': filename,\n",
    "        'book_number': info['book_number'],\n",
    "        'title': info['title'],\n",
    "        'pages': info['pages'],\n",
    "        'year': info['year'],\n",
    "        'text': clean,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Total livres extraits: {len(books_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Segmentation en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Segmente le texte en phrases.\n",
    "    \n",
    "    Note: Pour une segmentation plus pr√©cise, on utilisera spaCy dans le notebook 02.\n",
    "    Ici, on fait une segmentation simple bas√©e sur les points.\n",
    "    \"\"\"\n",
    "    # Regex simple pour diviser sur . ! ? suivis d'espace et majuscule\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z√Ä-√ú])', text)\n",
    "    \n",
    "    # Filtrer les phrases trop courtes\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le corpus de phrases\n",
    "all_sentences = []\n",
    "\n",
    "for book in books_data:\n",
    "    print(f\"\\nüìù Segmentation: {book['title']}\")\n",
    "    \n",
    "    sentences = segment_into_sentences(book['text'])\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        all_sentences.append({\n",
    "            'book_number': book['book_number'],\n",
    "            'book_title': book['title'],\n",
    "            'sentence_id': idx,\n",
    "            'text': sentence,\n",
    "            'length': len(sentence)\n",
    "        })\n",
    "    \n",
    "    print(f\"  ‚úÖ {len(sentences):,} phrases extraites\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total phrases: {len(all_sentences):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er DataFrame et exporter en parquet\n",
    "df_sentences = pd.DataFrame(all_sentences)\n",
    "\n",
    "print(\"\\nüìä Aper√ßu du DataFrame:\")\n",
    "print(df_sentences.head())\n",
    "print(f\"\\nüìè Shape: {df_sentences.shape}\")\n",
    "print(f\"\\nüìà Statistiques:\")\n",
    "print(df_sentences.groupby('book_title')['sentence_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en parquet\n",
    "output_path = DATA_DIR / 'sentences.parquet'\n",
    "df_sentences.to_parquet(output_path, index=False)\n",
    "print(f\"‚úÖ Donn√©es export√©es: {output_path}\")\n",
    "print(f\"üì¶ Taille fichier: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'books': [\n",
    "        {\n",
    "            'book_number': book['book_number'],\n",
    "            'title': book['title'],\n",
    "            'pages': book['pages'],\n",
    "            'year': book['year'],\n",
    "            'word_count': book['word_count'],\n",
    "            'char_count': book['char_count']\n",
    "        }\n",
    "        for book in books_data\n",
    "    ],\n",
    "    'total_sentences': len(all_sentences),\n",
    "    'extraction_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = DATA_DIR / 'book_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ M√©tadonn√©es export√©es: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier que les donn√©es peuvent √™tre relues\n",
    "df_test = pd.read_parquet(output_path)\n",
    "print(f\"‚úÖ V√©rification lecture parquet: {df_test.shape}\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"\\nüìñ Exemples de phrases:\")\n",
    "for i in range(min(5, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    print(f\"\\nLivre {row['book_number']} - {row['book_title']}\")\n",
    "    print(f\"  {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "Ce notebook a:\n",
    "1. ‚úÖ Extrait le texte des 7 livres Harry Potter\n",
    "2. ‚úÖ Nettoy√© et normalis√© le texte\n",
    "3. ‚úÖ Segment√© en phrases\n",
    "4. ‚úÖ Export√© en format parquet\n",
    "\n",
    "**Prochaine √©tape**: Notebook 02 - Pipeline NLP pour NER, cor√©f√©rence et attribution de locuteur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
