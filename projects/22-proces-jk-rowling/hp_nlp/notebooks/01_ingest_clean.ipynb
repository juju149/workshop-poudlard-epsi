{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 Notebook 1 - Ingestion et Nettoyage des Textes Harry Potter\n",
    "\n",
    "Ce notebook extrait et nettoie les textes des livres Harry Potter depuis les PDFs.\n",
    "\n",
    "## Objectifs\n",
    "1. Extraire le texte brut des 7 livres PDF\n",
    "2. Nettoyer et normaliser le texte\n",
    "3. Segmenter par phrases et dialogues\n",
    "4. Exporter en format parquet pour analyse ultérieure\n",
    "\n",
    "## Sorties\n",
    "- `data/sentences.parquet` : corpus segmenté par phrases\n",
    "- `data/book_metadata.json` : métadonnées des livres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.113804Z",
     "iopub.status.busy": "2025-10-15T13:11:50.113804Z",
     "iopub.status.idle": "2025-10-15T13:11:50.520873Z",
     "shell.execute_reply": "2025-10-15T13:11:50.520873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Repo root: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\n",
      "📁 Project dir: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\n",
      "📚 Books directory: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\context\\books\n",
      "💾 Data output: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration des chemins\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Trouver la racine du dépôt en cherchant le dossier 'context'\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'context').is_dir():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "PROJECT_DIR = None\n",
    "# Trouver le répertoire de projet (contenant 'notebooks')\n",
    "for p in [NOTEBOOK_DIR] + list(NOTEBOOK_DIR.parents):\n",
    "    if (p / 'notebooks').is_dir():\n",
    "        PROJECT_DIR = p\n",
    "        break\n",
    "if PROJECT_DIR is None:\n",
    "    PROJECT_DIR = NOTEBOOK_DIR\n",
    "\n",
    "BOOKS_DIR = REPO_ROOT / 'context' / 'books'\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "\n",
    "# Créer le dossier data si nécessaire\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Repo root: {REPO_ROOT}\")\n",
    "print(f\"📁 Project dir: {PROJECT_DIR}\")\n",
    "print(f\"📚 Books directory: {BOOKS_DIR}\")\n",
    "print(f\"💾 Data output: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.546525Z",
     "iopub.status.busy": "2025-10-15T13:11:50.546525Z",
     "iopub.status.idle": "2025-10-15T13:11:50.554942Z",
     "shell.execute_reply": "2025-10-15T13:11:50.554942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Nombre de livres configurés: 7\n"
     ]
    }
   ],
   "source": [
    "# Métadonnées des livres\n",
    "BOOK_INFO = {\n",
    "    \"harry-potter-1-lecole-des-sorciers.pdf\": {\n",
    "        \"title\": \"L'École des Sorciers\",\n",
    "        \"book_number\": 1,\n",
    "        \"pages\": 320,\n",
    "        \"year\": 1997\n",
    "    },\n",
    "    \"harry-potter-2-la-chambre-des-secrets.pdf\": {\n",
    "        \"title\": \"La Chambre des Secrets\",\n",
    "        \"book_number\": 2,\n",
    "        \"pages\": 360,\n",
    "        \"year\": 1998\n",
    "    },\n",
    "    \"harry-potter-3-le-prisonnier-dazkaban.pdf\": {\n",
    "        \"title\": \"Le Prisonnier d'Azkaban\",\n",
    "        \"book_number\": 3,\n",
    "        \"pages\": 420,\n",
    "        \"year\": 1999\n",
    "    },\n",
    "    \"harry-potter-4-la-coupe-de-feu.pdf\": {\n",
    "        \"title\": \"La Coupe de Feu\",\n",
    "        \"book_number\": 4,\n",
    "        \"pages\": 656,\n",
    "        \"year\": 2000\n",
    "    },\n",
    "    \"harry-potter-5-lordre-du-phoenix.pdf\": {\n",
    "        \"title\": \"L'Ordre du Phénix\",\n",
    "        \"book_number\": 5,\n",
    "        \"pages\": 980,\n",
    "        \"year\": 2003\n",
    "    },\n",
    "    \"harry-potter-6-le-prince-de-sang-mecc82lecc81.pdf\": {\n",
    "        \"title\": \"Le Prince de Sang-Mêlé\",\n",
    "        \"book_number\": 6,\n",
    "        \"pages\": 640,\n",
    "        \"year\": 2005\n",
    "    },\n",
    "    \"harry-potter-7-les-reliques-de-la-mort.pdf\": {\n",
    "        \"title\": \"Les Reliques de la Mort\",\n",
    "        \"book_number\": 7,\n",
    "        \"pages\": 800,\n",
    "        \"year\": 2007\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"📚 Nombre de livres configurés: {len(BOOK_INFO)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction du texte depuis les PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.554942Z",
     "iopub.status.busy": "2025-10-15T13:11:50.554942Z",
     "iopub.status.idle": "2025-10-15T13:11:50.563464Z",
     "shell.execute_reply": "2025-10-15T13:11:50.563464Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"  📄 Pages: {len(pdf_reader.pages)}\")\n",
    "            \n",
    "            for page_num, page in enumerate(tqdm(pdf_reader.pages, desc=\"Extraction\")):\n",
    "                try:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠️  Erreur page {page_num}: {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Erreur lecture PDF: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte extrait.\"\"\"\n",
    "    # Supprimer les sauts de ligne excessifs\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Normaliser les guillemets\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201c', '«').replace('\\u201d', '»')\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.566047Z",
     "iopub.status.busy": "2025-10-15T13:11:50.566047Z",
     "iopub.status.idle": "2025-10-15T13:13:38.098556Z",
     "shell.execute_reply": "2025-10-15T13:13:38.098556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📖 Extraction: L'École des Sorciers\n",
      "  📄 Pages: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction:   0%|          | 0/208 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 208/208 [00:08<00:00, 25.00it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 85,803\n",
      "  ✅ Caractères: 501,556\n",
      "\n",
      "📖 Extraction: La Chambre des Secrets\n",
      "  📄 Pages: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 177/177 [00:08<00:00, 20.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 92,206\n",
      "  ✅ Caractères: 543,799\n",
      "\n",
      "📖 Extraction: Le Prisonnier d'Azkaban\n",
      "  📄 Pages: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 288/288 [00:11<00:00, 25.73it/s]\n",
      "Extraction: 100%|██████████| 288/288 [00:11<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 121,640\n",
      "  ✅ Caractères: 711,797\n",
      "\n",
      "📖 Extraction: La Coupe de Feu\n",
      "  📄 Pages: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 386/386 [00:19<00:00, 19.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 221,669\n",
      "  ✅ Caractères: 1,290,495\n",
      "\n",
      "📖 Extraction: L'Ordre du Phénix\n",
      "  📄 Pages: 694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 694/694 [00:27<00:00, 25.55it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 295,484\n",
      "  ✅ Caractères: 1,715,842\n",
      "\n",
      "📖 Extraction: Le Prince de Sang-Mêlé\n",
      "  📄 Pages: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 415/415 [00:17<00:00, 24.19it/s]\n",
      "Extraction: 100%|██████████| 415/415 [00:17<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 193,852\n",
      "  ✅ Caractères: 1,140,529\n",
      "\n",
      "📖 Extraction: Les Reliques de la Mort\n",
      "  📄 Pages: 541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|██████████| 541/541 [00:20<00:00, 25.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Mots: 230,278\n",
      "  ✅ Caractères: 1,340,353\n",
      "\n",
      "✅ Total livres extraits: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraire le texte de tous les livres\n",
    "books_data = []\n",
    "\n",
    "for filename, info in BOOK_INFO.items():\n",
    "    pdf_path = BOOKS_DIR / filename\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"⚠️  Livre non trouvé: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📖 Extraction: {info['title']}\")\n",
    "    \n",
    "    # Extraire et nettoyer\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    clean = clean_text(raw_text)\n",
    "    \n",
    "    # Statistiques de base\n",
    "    word_count = len(clean.split())\n",
    "    char_count = len(clean)\n",
    "    \n",
    "    print(f\"  ✅ Mots: {word_count:,}\")\n",
    "    print(f\"  ✅ Caractères: {char_count:,}\")\n",
    "    \n",
    "    books_data.append({\n",
    "        'filename': filename,\n",
    "        'book_number': info['book_number'],\n",
    "        'title': info['title'],\n",
    "        'pages': info['pages'],\n",
    "        'year': info['year'],\n",
    "        'text': clean,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count\n",
    "    })\n",
    "\n",
    "print(f\"\\n✅ Total livres extraits: {len(books_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Segmentation en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.098556Z",
     "iopub.status.busy": "2025-10-15T13:13:38.098556Z",
     "iopub.status.idle": "2025-10-15T13:13:38.106569Z",
     "shell.execute_reply": "2025-10-15T13:13:38.106569Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Segmente le texte en phrases.\n",
    "    \n",
    "    Note: Pour une segmentation plus précise, on utilisera spaCy dans le notebook 02.\n",
    "    Ici, on fait une segmentation simple basée sur les points.\n",
    "    \"\"\"\n",
    "    # Regex simple pour diviser sur . ! ? suivis d'espace et majuscule\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-ZÀ-Ü])', text)\n",
    "    \n",
    "    # Filtrer les phrases trop courtes\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.106569Z",
     "iopub.status.busy": "2025-10-15T13:13:38.106569Z",
     "iopub.status.idle": "2025-10-15T13:13:38.262206Z",
     "shell.execute_reply": "2025-10-15T13:13:38.262206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Segmentation: L'École des Sorciers\n",
      "  ✅ 4,934 phrases extraites\n",
      "\n",
      "📝 Segmentation: La Chambre des Secrets\n",
      "  ✅ 4,951 phrases extraites\n",
      "\n",
      "📝 Segmentation: Le Prisonnier d'Azkaban\n",
      "  ✅ 6,742 phrases extraites\n",
      "\n",
      "📝 Segmentation: La Coupe de Feu\n",
      "  ✅ 11,134 phrases extraites\n",
      "\n",
      "📝 Segmentation: L'Ordre du Phénix\n",
      "  ✅ 12,008 phrases extraites\n",
      "\n",
      "📝 Segmentation: Le Prince de Sang-Mêlé\n",
      "  ✅ 8,361 phrases extraites\n",
      "\n",
      "📝 Segmentation: Les Reliques de la Mort\n",
      "  ✅ 10,524 phrases extraites\n",
      "\n",
      "✅ Total phrases: 58,654\n"
     ]
    }
   ],
   "source": [
    "# Créer le corpus de phrases\n",
    "all_sentences = []\n",
    "\n",
    "for book in books_data:\n",
    "    print(f\"\\n📝 Segmentation: {book['title']}\")\n",
    "    \n",
    "    sentences = segment_into_sentences(book['text'])\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        all_sentences.append({\n",
    "            'book_number': book['book_number'],\n",
    "            'book_title': book['title'],\n",
    "            'sentence_id': idx,\n",
    "            'text': sentence,\n",
    "            'length': len(sentence)\n",
    "        })\n",
    "    \n",
    "    print(f\"  ✅ {len(sentences):,} phrases extraites\")\n",
    "\n",
    "print(f\"\\n✅ Total phrases: {len(all_sentences):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.265222Z",
     "iopub.status.busy": "2025-10-15T13:13:38.262206Z",
     "iopub.status.idle": "2025-10-15T13:13:38.319129Z",
     "shell.execute_reply": "2025-10-15T13:13:38.319129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Aperçu du DataFrame:\n",
      "   book_number            book_title  sentence_id  \\\n",
      "0            1  L'École des Sorciers            0   \n",
      "1            1  L'École des Sorciers            1   \n",
      "2            1  L'École des Sorciers            2   \n",
      "3            1  L'École des Sorciers            3   \n",
      "4            1  L'École des Sorciers            4   \n",
      "\n",
      "                                                text  length  \n",
      "0                                    L'auteure\\nJ.K.      14  \n",
      "1  Rowling\\test\\tnée\\ten\\t1967\\tet\\ta\\tpassé\\tson...     101  \n",
      "2  Elle\\ta\\tsuivi\\tdes\\tétudes\\tà\\tl'université\\t...     100  \n",
      "3  Elle\\ta\\tensuite\\ttravaillé\\tquelque\\ttemps\\tà...     122  \n",
      "4  C'est\\ten\\t1990\\tque\\tl'idée\\tde\\tHarry\\tPotte...     160  \n",
      "\n",
      "📏 Shape: (58654, 5)\n",
      "\n",
      "📈 Statistiques:\n",
      "book_title\n",
      "L'Ordre du Phénix          12008\n",
      "L'École des Sorciers        4934\n",
      "La Chambre des Secrets      4951\n",
      "La Coupe de Feu            11134\n",
      "Le Prince de Sang-Mêlé      8361\n",
      "Le Prisonnier d'Azkaban     6742\n",
      "Les Reliques de la Mort    10524\n",
      "Name: sentence_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Créer DataFrame et préparer l'export en parquet\n",
    "df_sentences = pd.DataFrame(all_sentences)\n",
    "\n",
    "# Définir le chemin de sortie même si on n'exporte pas (utilisé plus tard)\n",
    "output_path = DATA_DIR / 'sentences.parquet'\n",
    "\n",
    "try:\n",
    "    if df_sentences.empty:\n",
    "        print(\"\\n⚠️ Aucune phrase extraite — vérifiez que les PDFs existent dans the 'context/books' directory.\")\n",
    "        print(f\"  Books dir checked: {BOOKS_DIR}\")\n",
    "        print(\"✅ Aucune exportation réalisée.\")\n",
    "    else:\n",
    "        print(\"\\n📊 Aperçu du DataFrame:\")\n",
    "        print(df_sentences.head())\n",
    "        print(f\"\\n📏 Shape: {df_sentences.shape}\")\n",
    "        print(f\"\\n📈 Statistiques:\")\n",
    "        try:\n",
    "            print(df_sentences.groupby('book_title')['sentence_id'].count())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Impossible d'obtenir les statistiques de groupby: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erreur lors de la préparation du DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.319129Z",
     "iopub.status.busy": "2025-10-15T13:13:38.319129Z",
     "iopub.status.idle": "2025-10-15T13:13:38.598152Z",
     "shell.execute_reply": "2025-10-15T13:13:38.598152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données exportées: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\sentences.parquet\n",
      "📦 Taille fichier: 4.74 MB\n"
     ]
    }
   ],
   "source": [
    "# Exporter en parquet si des phrases existent\n",
    "if 'df_sentences' in globals() and not df_sentences.empty:\n",
    "    output_path = DATA_DIR / 'sentences.parquet'\n",
    "    df_sentences.to_parquet(output_path, index=False)\n",
    "    print(f\"✅ Données exportées: {output_path}\")\n",
    "    try:\n",
    "        print(f\"📦 Taille fichier: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"⚠️ Pas d'export car aucun texte n'a été extrait.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.598152Z",
     "iopub.status.busy": "2025-10-15T13:13:38.598152Z",
     "iopub.status.idle": "2025-10-15T13:13:38.608153Z",
     "shell.execute_reply": "2025-10-15T13:13:38.608153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Métadonnées exportées: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\book_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Exporter les métadonnées\n",
    "metadata = {\n",
    "    'books': [\n",
    "        {\n",
    "            'book_number': book['book_number'],\n",
    "            'title': book['title'],\n",
    "            'pages': book['pages'],\n",
    "            'year': book['year'],\n",
    "            'word_count': book['word_count'],\n",
    "            'char_count': book['char_count']\n",
    "        }\n",
    "        for book in books_data\n",
    "    ],\n",
    "    'total_sentences': len(all_sentences),\n",
    "    'extraction_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = DATA_DIR / 'book_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Métadonnées exportées: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.611280Z",
     "iopub.status.busy": "2025-10-15T13:13:38.611280Z",
     "iopub.status.idle": "2025-10-15T13:13:38.934602Z",
     "shell.execute_reply": "2025-10-15T13:13:38.934602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vérification lecture parquet: (58654, 5)\n",
      "\n",
      "📖 Exemples de phrases:\n",
      "\n",
      "Livre 1 - L'École des Sorciers\n",
      "  L'auteure\n",
      "J.K....\n",
      "\n",
      "Livre 1 - L'École des Sorciers\n",
      "  Rowling\test\tnée\ten\t1967\tet\ta\tpassé\tson\tenfance\tà\tChepstow,\tdans\tle\tcomté\tde\tGwent,\tau\tpays\tde\n",
      "Galles....\n",
      "\n",
      "Livre 1 - L'École des Sorciers\n",
      "  Elle\ta\tsuivi\tdes\tétudes\tà\tl'université\td'Exeter\tet\test\tdiplômée\ten\tlangue\tet\tlittérature\tfrançaises....\n",
      "\n",
      "Livre 1 - L'École des Sorciers\n",
      "  Elle\ta\tensuite\ttravaillé\tquelque\ttemps\tà\tLondres\tau\tsein\tde\tl'association\tAmnesty\tInternational\tet\ta\n",
      "enseigné\tle\tfrançais....\n",
      "\n",
      "Livre 1 - L'École des Sorciers\n",
      "  C'est\ten\t1990\tque\tl'idée\tde\tHarry\tPotter\tet\tde\tson\técole\tde\tmagiciens\ta\tcommencé\tà\tgermer\tdans\tson\n",
      "esprit,\talors\tqu'elle\tattendait\tun\ttrain\tqui\tavait\tdu\tretard....\n"
     ]
    }
   ],
   "source": [
    "# Vérifier que les données peuvent être relues (si le fichier existe)\n",
    "if output_path.exists():\n",
    "    df_test = pd.read_parquet(output_path)\n",
    "    print(f\"✅ Vérification lecture parquet: {df_test.shape}\")\n",
    "\n",
    "    # Afficher quelques exemples\n",
    "    print(\"\\n📖 Exemples de phrases:\")\n",
    "    for i in range(min(5, len(df_test))):\n",
    "        row = df_test.iloc[i]\n",
    "        print(f\"\\nLivre {row['book_number']} - {row['book_title']}\")\n",
    "        print(f\"  {row['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"⚠️ Aucun fichier parquet trouvé — pas d'extraction valide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Résumé\n",
    "\n",
    "Ce notebook a:\n",
    "1. ✅ Extrait le texte des 7 livres Harry Potter\n",
    "2. ✅ Nettoyé et normalisé le texte\n",
    "3. ✅ Segmenté en phrases\n",
    "4. ✅ Exporté en format parquet\n",
    "\n",
    "**Prochaine étape**: Notebook 02 - Pipeline NLP pour NER, coréférence et attribution de locuteur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
