{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Notebook 1 - Ingestion et Nettoyage des Textes Harry Potter\n",
    "\n",
    "Ce notebook extrait et nettoie les textes des livres Harry Potter depuis les PDFs.\n",
    "\n",
    "## Objectifs\n",
    "1. Extraire le texte brut des 7 livres PDF\n",
    "2. Nettoyer et normaliser le texte\n",
    "3. Segmenter par phrases et dialogues\n",
    "4. Exporter en format parquet pour analyse ult√©rieure\n",
    "\n",
    "## Sorties\n",
    "- `data/sentences.parquet` : corpus segment√© par phrases\n",
    "- `data/book_metadata.json` : m√©tadonn√©es des livres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.113804Z",
     "iopub.status.busy": "2025-10-15T13:11:50.113804Z",
     "iopub.status.idle": "2025-10-15T13:11:50.520873Z",
     "shell.execute_reply": "2025-10-15T13:11:50.520873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Repo root: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\n",
      "üìÅ Project dir: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\n",
      "üìö Books directory: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\context\\books\n",
      "üíæ Data output: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration des chemins\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Trouver la racine du d√©p√¥t en cherchant le dossier 'context'\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / 'context').is_dir():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "REPO_ROOT = find_repo_root(NOTEBOOK_DIR)\n",
    "PROJECT_DIR = None\n",
    "# Trouver le r√©pertoire de projet (contenant 'notebooks')\n",
    "for p in [NOTEBOOK_DIR] + list(NOTEBOOK_DIR.parents):\n",
    "    if (p / 'notebooks').is_dir():\n",
    "        PROJECT_DIR = p\n",
    "        break\n",
    "if PROJECT_DIR is None:\n",
    "    PROJECT_DIR = NOTEBOOK_DIR\n",
    "\n",
    "BOOKS_DIR = REPO_ROOT / 'context' / 'books'\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "\n",
    "# Cr√©er le dossier data si n√©cessaire\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Repo root: {REPO_ROOT}\")\n",
    "print(f\"üìÅ Project dir: {PROJECT_DIR}\")\n",
    "print(f\"üìö Books directory: {BOOKS_DIR}\")\n",
    "print(f\"üíæ Data output: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.546525Z",
     "iopub.status.busy": "2025-10-15T13:11:50.546525Z",
     "iopub.status.idle": "2025-10-15T13:11:50.554942Z",
     "shell.execute_reply": "2025-10-15T13:11:50.554942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Nombre de livres configur√©s: 7\n"
     ]
    }
   ],
   "source": [
    "# M√©tadonn√©es des livres\n",
    "BOOK_INFO = {\n",
    "    \"harry-potter-1-lecole-des-sorciers.pdf\": {\n",
    "        \"title\": \"L'√âcole des Sorciers\",\n",
    "        \"book_number\": 1,\n",
    "        \"pages\": 320,\n",
    "        \"year\": 1997\n",
    "    },\n",
    "    \"harry-potter-2-la-chambre-des-secrets.pdf\": {\n",
    "        \"title\": \"La Chambre des Secrets\",\n",
    "        \"book_number\": 2,\n",
    "        \"pages\": 360,\n",
    "        \"year\": 1998\n",
    "    },\n",
    "    \"harry-potter-3-le-prisonnier-dazkaban.pdf\": {\n",
    "        \"title\": \"Le Prisonnier d'Azkaban\",\n",
    "        \"book_number\": 3,\n",
    "        \"pages\": 420,\n",
    "        \"year\": 1999\n",
    "    },\n",
    "    \"harry-potter-4-la-coupe-de-feu.pdf\": {\n",
    "        \"title\": \"La Coupe de Feu\",\n",
    "        \"book_number\": 4,\n",
    "        \"pages\": 656,\n",
    "        \"year\": 2000\n",
    "    },\n",
    "    \"harry-potter-5-lordre-du-phoenix.pdf\": {\n",
    "        \"title\": \"L'Ordre du Ph√©nix\",\n",
    "        \"book_number\": 5,\n",
    "        \"pages\": 980,\n",
    "        \"year\": 2003\n",
    "    },\n",
    "    \"harry-potter-6-le-prince-de-sang-mecc82lecc81.pdf\": {\n",
    "        \"title\": \"Le Prince de Sang-M√™l√©\",\n",
    "        \"book_number\": 6,\n",
    "        \"pages\": 640,\n",
    "        \"year\": 2005\n",
    "    },\n",
    "    \"harry-potter-7-les-reliques-de-la-mort.pdf\": {\n",
    "        \"title\": \"Les Reliques de la Mort\",\n",
    "        \"book_number\": 7,\n",
    "        \"pages\": 800,\n",
    "        \"year\": 2007\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìö Nombre de livres configur√©s: {len(BOOK_INFO)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction du texte depuis les PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.554942Z",
     "iopub.status.busy": "2025-10-15T13:11:50.554942Z",
     "iopub.status.idle": "2025-10-15T13:11:50.563464Z",
     "shell.execute_reply": "2025-10-15T13:11:50.563464Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extrait le texte d'un fichier PDF.\"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            print(f\"  üìÑ Pages: {len(pdf_reader.pages)}\")\n",
    "            \n",
    "            for page_num, page in enumerate(tqdm(pdf_reader.pages, desc=\"Extraction\")):\n",
    "                try:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Erreur page {page_num}: {e}\")\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erreur lecture PDF: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte extrait.\"\"\"\n",
    "    # Supprimer les sauts de ligne excessifs\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    \n",
    "    # Normaliser les guillemets\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201c', '¬´').replace('\\u201d', '¬ª')\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:11:50.566047Z",
     "iopub.status.busy": "2025-10-15T13:11:50.566047Z",
     "iopub.status.idle": "2025-10-15T13:13:38.098556Z",
     "shell.execute_reply": "2025-10-15T13:13:38.098556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Extraction: L'√âcole des Sorciers\n",
      "  üìÑ Pages: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction:   0%|          | 0/208 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 208/208 [00:08<00:00, 25.00it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 85,803\n",
      "  ‚úÖ Caract√®res: 501,556\n",
      "\n",
      "üìñ Extraction: La Chambre des Secrets\n",
      "  üìÑ Pages: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 177/177 [00:08<00:00, 20.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 92,206\n",
      "  ‚úÖ Caract√®res: 543,799\n",
      "\n",
      "üìñ Extraction: Le Prisonnier d'Azkaban\n",
      "  üìÑ Pages: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 288/288 [00:11<00:00, 25.73it/s]\n",
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 288/288 [00:11<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 121,640\n",
      "  ‚úÖ Caract√®res: 711,797\n",
      "\n",
      "üìñ Extraction: La Coupe de Feu\n",
      "  üìÑ Pages: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:19<00:00, 19.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 221,669\n",
      "  ‚úÖ Caract√®res: 1,290,495\n",
      "\n",
      "üìñ Extraction: L'Ordre du Ph√©nix\n",
      "  üìÑ Pages: 694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 694/694 [00:27<00:00, 25.55it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 295,484\n",
      "  ‚úÖ Caract√®res: 1,715,842\n",
      "\n",
      "üìñ Extraction: Le Prince de Sang-M√™l√©\n",
      "  üìÑ Pages: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 415/415 [00:17<00:00, 24.19it/s]\n",
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 415/415 [00:17<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 193,852\n",
      "  ‚úÖ Caract√®res: 1,140,529\n",
      "\n",
      "üìñ Extraction: Les Reliques de la Mort\n",
      "  üìÑ Pages: 541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541/541 [00:20<00:00, 25.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Mots: 230,278\n",
      "  ‚úÖ Caract√®res: 1,340,353\n",
      "\n",
      "‚úÖ Total livres extraits: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraire le texte de tous les livres\n",
    "books_data = []\n",
    "\n",
    "for filename, info in BOOK_INFO.items():\n",
    "    pdf_path = BOOKS_DIR / filename\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Livre non trouv√©: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìñ Extraction: {info['title']}\")\n",
    "    \n",
    "    # Extraire et nettoyer\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    clean = clean_text(raw_text)\n",
    "    \n",
    "    # Statistiques de base\n",
    "    word_count = len(clean.split())\n",
    "    char_count = len(clean)\n",
    "    \n",
    "    print(f\"  ‚úÖ Mots: {word_count:,}\")\n",
    "    print(f\"  ‚úÖ Caract√®res: {char_count:,}\")\n",
    "    \n",
    "    books_data.append({\n",
    "        'filename': filename,\n",
    "        'book_number': info['book_number'],\n",
    "        'title': info['title'],\n",
    "        'pages': info['pages'],\n",
    "        'year': info['year'],\n",
    "        'text': clean,\n",
    "        'word_count': word_count,\n",
    "        'char_count': char_count\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Total livres extraits: {len(books_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Segmentation en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.098556Z",
     "iopub.status.busy": "2025-10-15T13:13:38.098556Z",
     "iopub.status.idle": "2025-10-15T13:13:38.106569Z",
     "shell.execute_reply": "2025-10-15T13:13:38.106569Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Segmente le texte en phrases.\n",
    "    \n",
    "    Note: Pour une segmentation plus pr√©cise, on utilisera spaCy dans le notebook 02.\n",
    "    Ici, on fait une segmentation simple bas√©e sur les points.\n",
    "    \"\"\"\n",
    "    # Regex simple pour diviser sur . ! ? suivis d'espace et majuscule\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z√Ä-√ú])', text)\n",
    "    \n",
    "    # Filtrer les phrases trop courtes\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.106569Z",
     "iopub.status.busy": "2025-10-15T13:13:38.106569Z",
     "iopub.status.idle": "2025-10-15T13:13:38.262206Z",
     "shell.execute_reply": "2025-10-15T13:13:38.262206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Segmentation: L'√âcole des Sorciers\n",
      "  ‚úÖ 4,934 phrases extraites\n",
      "\n",
      "üìù Segmentation: La Chambre des Secrets\n",
      "  ‚úÖ 4,951 phrases extraites\n",
      "\n",
      "üìù Segmentation: Le Prisonnier d'Azkaban\n",
      "  ‚úÖ 6,742 phrases extraites\n",
      "\n",
      "üìù Segmentation: La Coupe de Feu\n",
      "  ‚úÖ 11,134 phrases extraites\n",
      "\n",
      "üìù Segmentation: L'Ordre du Ph√©nix\n",
      "  ‚úÖ 12,008 phrases extraites\n",
      "\n",
      "üìù Segmentation: Le Prince de Sang-M√™l√©\n",
      "  ‚úÖ 8,361 phrases extraites\n",
      "\n",
      "üìù Segmentation: Les Reliques de la Mort\n",
      "  ‚úÖ 10,524 phrases extraites\n",
      "\n",
      "‚úÖ Total phrases: 58,654\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er le corpus de phrases\n",
    "all_sentences = []\n",
    "\n",
    "for book in books_data:\n",
    "    print(f\"\\nüìù Segmentation: {book['title']}\")\n",
    "    \n",
    "    sentences = segment_into_sentences(book['text'])\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        all_sentences.append({\n",
    "            'book_number': book['book_number'],\n",
    "            'book_title': book['title'],\n",
    "            'sentence_id': idx,\n",
    "            'text': sentence,\n",
    "            'length': len(sentence)\n",
    "        })\n",
    "    \n",
    "    print(f\"  ‚úÖ {len(sentences):,} phrases extraites\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total phrases: {len(all_sentences):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.265222Z",
     "iopub.status.busy": "2025-10-15T13:13:38.262206Z",
     "iopub.status.idle": "2025-10-15T13:13:38.319129Z",
     "shell.execute_reply": "2025-10-15T13:13:38.319129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aper√ßu du DataFrame:\n",
      "   book_number            book_title  sentence_id  \\\n",
      "0            1  L'√âcole des Sorciers            0   \n",
      "1            1  L'√âcole des Sorciers            1   \n",
      "2            1  L'√âcole des Sorciers            2   \n",
      "3            1  L'√âcole des Sorciers            3   \n",
      "4            1  L'√âcole des Sorciers            4   \n",
      "\n",
      "                                                text  length  \n",
      "0                                    L'auteure\\nJ.K.      14  \n",
      "1  Rowling\\test\\tn√©e\\ten\\t1967\\tet\\ta\\tpass√©\\tson...     101  \n",
      "2  Elle\\ta\\tsuivi\\tdes\\t√©tudes\\t√†\\tl'universit√©\\t...     100  \n",
      "3  Elle\\ta\\tensuite\\ttravaill√©\\tquelque\\ttemps\\t√†...     122  \n",
      "4  C'est\\ten\\t1990\\tque\\tl'id√©e\\tde\\tHarry\\tPotte...     160  \n",
      "\n",
      "üìè Shape: (58654, 5)\n",
      "\n",
      "üìà Statistiques:\n",
      "book_title\n",
      "L'Ordre du Ph√©nix          12008\n",
      "L'√âcole des Sorciers        4934\n",
      "La Chambre des Secrets      4951\n",
      "La Coupe de Feu            11134\n",
      "Le Prince de Sang-M√™l√©      8361\n",
      "Le Prisonnier d'Azkaban     6742\n",
      "Les Reliques de la Mort    10524\n",
      "Name: sentence_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er DataFrame et pr√©parer l'export en parquet\n",
    "df_sentences = pd.DataFrame(all_sentences)\n",
    "\n",
    "# D√©finir le chemin de sortie m√™me si on n'exporte pas (utilis√© plus tard)\n",
    "output_path = DATA_DIR / 'sentences.parquet'\n",
    "\n",
    "try:\n",
    "    if df_sentences.empty:\n",
    "        print(\"\\n‚ö†Ô∏è Aucune phrase extraite ‚Äî v√©rifiez que les PDFs existent dans the 'context/books' directory.\")\n",
    "        print(f\"  Books dir checked: {BOOKS_DIR}\")\n",
    "        print(\"‚úÖ Aucune exportation r√©alis√©e.\")\n",
    "    else:\n",
    "        print(\"\\nüìä Aper√ßu du DataFrame:\")\n",
    "        print(df_sentences.head())\n",
    "        print(f\"\\nüìè Shape: {df_sentences.shape}\")\n",
    "        print(f\"\\nüìà Statistiques:\")\n",
    "        try:\n",
    "            print(df_sentences.groupby('book_title')['sentence_id'].count())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Impossible d'obtenir les statistiques de groupby: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur lors de la pr√©paration du DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.319129Z",
     "iopub.status.busy": "2025-10-15T13:13:38.319129Z",
     "iopub.status.idle": "2025-10-15T13:13:38.598152Z",
     "shell.execute_reply": "2025-10-15T13:13:38.598152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es export√©es: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\sentences.parquet\n",
      "üì¶ Taille fichier: 4.74 MB\n"
     ]
    }
   ],
   "source": [
    "# Exporter en parquet si des phrases existent\n",
    "if 'df_sentences' in globals() and not df_sentences.empty:\n",
    "    output_path = DATA_DIR / 'sentences.parquet'\n",
    "    df_sentences.to_parquet(output_path, index=False)\n",
    "    print(f\"‚úÖ Donn√©es export√©es: {output_path}\")\n",
    "    try:\n",
    "        print(f\"üì¶ Taille fichier: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas d'export car aucun texte n'a √©t√© extrait.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.598152Z",
     "iopub.status.busy": "2025-10-15T13:13:38.598152Z",
     "iopub.status.idle": "2025-10-15T13:13:38.608153Z",
     "shell.execute_reply": "2025-10-15T13:13:38.608153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ M√©tadonn√©es export√©es: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\book_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Exporter les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'books': [\n",
    "        {\n",
    "            'book_number': book['book_number'],\n",
    "            'title': book['title'],\n",
    "            'pages': book['pages'],\n",
    "            'year': book['year'],\n",
    "            'word_count': book['word_count'],\n",
    "            'char_count': book['char_count']\n",
    "        }\n",
    "        for book in books_data\n",
    "    ],\n",
    "    'total_sentences': len(all_sentences),\n",
    "    'extraction_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = DATA_DIR / 'book_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ M√©tadonn√©es export√©es: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:38.611280Z",
     "iopub.status.busy": "2025-10-15T13:13:38.611280Z",
     "iopub.status.idle": "2025-10-15T13:13:38.934602Z",
     "shell.execute_reply": "2025-10-15T13:13:38.934602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ V√©rification lecture parquet: (58654, 5)\n",
      "\n",
      "üìñ Exemples de phrases:\n",
      "\n",
      "Livre 1 - L'√âcole des Sorciers\n",
      "  L'auteure\n",
      "J.K....\n",
      "\n",
      "Livre 1 - L'√âcole des Sorciers\n",
      "  Rowling\test\tn√©e\ten\t1967\tet\ta\tpass√©\tson\tenfance\t√†\tChepstow,\tdans\tle\tcomt√©\tde\tGwent,\tau\tpays\tde\n",
      "Galles....\n",
      "\n",
      "Livre 1 - L'√âcole des Sorciers\n",
      "  Elle\ta\tsuivi\tdes\t√©tudes\t√†\tl'universit√©\td'Exeter\tet\test\tdipl√¥m√©e\ten\tlangue\tet\tlitt√©rature\tfran√ßaises....\n",
      "\n",
      "Livre 1 - L'√âcole des Sorciers\n",
      "  Elle\ta\tensuite\ttravaill√©\tquelque\ttemps\t√†\tLondres\tau\tsein\tde\tl'association\tAmnesty\tInternational\tet\ta\n",
      "enseign√©\tle\tfran√ßais....\n",
      "\n",
      "Livre 1 - L'√âcole des Sorciers\n",
      "  C'est\ten\t1990\tque\tl'id√©e\tde\tHarry\tPotter\tet\tde\tson\t√©cole\tde\tmagiciens\ta\tcommenc√©\t√†\tgermer\tdans\tson\n",
      "esprit,\talors\tqu'elle\tattendait\tun\ttrain\tqui\tavait\tdu\tretard....\n"
     ]
    }
   ],
   "source": [
    "# V√©rifier que les donn√©es peuvent √™tre relues (si le fichier existe)\n",
    "if output_path.exists():\n",
    "    df_test = pd.read_parquet(output_path)\n",
    "    print(f\"‚úÖ V√©rification lecture parquet: {df_test.shape}\")\n",
    "\n",
    "    # Afficher quelques exemples\n",
    "    print(\"\\nüìñ Exemples de phrases:\")\n",
    "    for i in range(min(5, len(df_test))):\n",
    "        row = df_test.iloc[i]\n",
    "        print(f\"\\nLivre {row['book_number']} - {row['book_title']}\")\n",
    "        print(f\"  {row['text'][:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun fichier parquet trouv√© ‚Äî pas d'extraction valide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "Ce notebook a:\n",
    "1. ‚úÖ Extrait le texte des 7 livres Harry Potter\n",
    "2. ‚úÖ Nettoy√© et normalis√© le texte\n",
    "3. ‚úÖ Segment√© en phrases\n",
    "4. ‚úÖ Export√© en format parquet\n",
    "\n",
    "**Prochaine √©tape**: Notebook 02 - Pipeline NLP pour NER, cor√©f√©rence et attribution de locuteur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
