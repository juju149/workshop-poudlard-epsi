{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Notebook 2 - Pipeline NLP Complet\n",
    "\n",
    "Ce notebook applique les techniques NLP avanc√©es sur le corpus Harry Potter.\n",
    "\n",
    "## Objectifs\n",
    "1. Tokenisation et POS tagging avec spaCy\n",
    "2. Named Entity Recognition (NER)\n",
    "3. R√©solution de cor√©f√©rence\n",
    "4. Attribution de locuteur pour les dialogues\n",
    "5. Index d'entit√©s principales\n",
    "\n",
    "## Entr√©es\n",
    "- `data/sentences.parquet` : corpus segment√©\n",
    "\n",
    "## Sorties\n",
    "- `data/nlp_processed.parquet` : texte avec annotations NLP\n",
    "- `data/entities_index.parquet` : index des entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Configuration\n",
    "NOTEBOOK_DIR = Path().absolute()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"üìÅ Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger le mod√®le spaCy\n",
    "\n",
    "Nous utilisons `fr_core_news_lg` pour une meilleure pr√©cision en fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le fran√ßais\n",
    "# Note: Installer avec: python -m spacy download fr_core_news_lg\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    print(\"‚úÖ Mod√®le fr_core_news_lg charg√©\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è  Mod√®le fr_core_news_lg non trouv√©, essai avec fr_core_news_sm\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        print(\"‚úÖ Mod√®le fr_core_news_sm charg√©\")\n",
    "    except OSError:\n",
    "        print(\"‚ùå Aucun mod√®le fran√ßais trouv√©. Installez avec:\")\n",
    "        print(\"   python -m spacy download fr_core_news_lg\")\n",
    "        raise\n",
    "\n",
    "# Configuration du pipeline\n",
    "print(f\"\\nüìã Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df = pd.read_parquet(DATA_DIR / 'sentences.parquet')\n",
    "print(f\"üìä Donn√©es charg√©es: {df.shape}\")\n",
    "print(f\"\\nüìñ Livres: {df['book_title'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER)\n",
    "\n",
    "Extraction des personnages et lieux mentionn√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personnages principaux √† tracker (canonique)\n",
    "MAIN_CHARACTERS = {\n",
    "    'Harry': ['Harry', 'Potter', 'Harry Potter'],\n",
    "    'Hermione': ['Hermione', 'Granger', 'Hermione Granger'],\n",
    "    'Ron': ['Ron', 'Ronald', 'Weasley', 'Ron Weasley'],\n",
    "    'Dumbledore': ['Dumbledore', 'Albus', 'directeur'],\n",
    "    'Rogue': ['Rogue', 'Severus', 'Snape', 'professeur Rogue'],\n",
    "    'Voldemort': ['Voldemort', 'Vous-Savez-Qui', 'Seigneur des T√©n√®bres'],\n",
    "    'Hagrid': ['Hagrid', 'Rubeus'],\n",
    "    'McGonagall': ['McGonagall', 'professeur McGonagall'],\n",
    "    'Sirius': ['Sirius', 'Black', 'Sirius Black'],\n",
    "    'Draco': ['Draco', 'Malfoy', 'Draco Malfoy']\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_character_name(text: str) -> Optional[str]:\n",
    "    \"\"\"Normalise un nom de personnage vers sa forme canonique.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for canonical, variants in MAIN_CHARACTERS.items():\n",
    "        for variant in variants:\n",
    "            if variant.lower() in text_lower:\n",
    "                return canonical\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les entit√©s d'une phrase\n",
    "def extract_entities(text: str, nlp_model) -> Dict:\n",
    "    \"\"\"Extrait les entit√©s nomm√©es d'un texte.\"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    entities = {\n",
    "        'persons': [],\n",
    "        'locations': [],\n",
    "        'organizations': []\n",
    "    }\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            normalized = normalize_character_name(ent.text)\n",
    "            if normalized:\n",
    "                entities['persons'].append(normalized)\n",
    "            else:\n",
    "                entities['persons'].append(ent.text)\n",
    "        elif ent.label_ == 'LOC':\n",
    "            entities['locations'].append(ent.text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            entities['organizations'].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "# Test sur un √©chantillon\n",
    "sample_text = df.iloc[100]['text']\n",
    "sample_entities = extract_entities(sample_text, nlp)\n",
    "print(\"\\nüîç Test extraction d'entit√©s:\")\n",
    "print(f\"Texte: {sample_text[:200]}...\")\n",
    "print(f\"Entit√©s: {sample_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attribution de locuteur\n",
    "\n",
    "D√©tection du personnage qui parle dans les dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_speaker(text: str) -> Optional[str]:\n",
    "    \"\"\"D√©tecte le locuteur dans une phrase contenant du dialogue.\n",
    "    \n",
    "    Utilise des heuristiques bas√©es sur les patterns de dialogue fran√ßais:\n",
    "    - ¬´ dialogue ¬ª dit X\n",
    "    - X dit : ¬´ dialogue ¬ª\n",
    "    - ‚Äî dialogue, dit X\n",
    "    \"\"\"\n",
    "    # Pattern: dit/d√©clara/r√©pondit + nom\n",
    "    patterns = [\n",
    "        r'(?:dit|d√©clara|r√©pondit|s\\'√©cria|demanda|murmura|hurla)\\s+([A-Z√Ä-√ú][a-z√†-√º]+)',\n",
    "        r'([A-Z√Ä-√ú][a-z√†-√º]+)\\s+(?:dit|d√©clara|r√©pondit|s\\'√©cria|demanda|murmura|hurla)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            speaker = match.group(1)\n",
    "            # Normaliser vers personnage principal si possible\n",
    "            normalized = normalize_character_name(speaker)\n",
    "            return normalized if normalized else speaker\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def is_dialogue(text: str) -> bool:\n",
    "    \"\"\"D√©tecte si une phrase contient du dialogue.\"\"\"\n",
    "    dialogue_markers = ['¬´', '¬ª', '‚Äî', 'dit', 'd√©clara', 'r√©pondit', \"s'√©cria\"]\n",
    "    return any(marker in text for marker in dialogue_markers)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    '¬´ Mais c\\'est impossible ! ¬ª dit Hermione.',\n",
    "    'Harry r√©pondit calmement : ¬´ Je sais. ¬ª',\n",
    "    '‚Äî Tu es s√ªr ? demanda Ron.',\n",
    "    'Il faisait beau ce jour-l√†.'\n",
    "]\n",
    "\n",
    "print(\"\\nüó£Ô∏è  Test d√©tection de locuteur:\")\n",
    "for sent in test_sentences:\n",
    "    speaker = detect_speaker(sent)\n",
    "    is_dial = is_dialogue(sent)\n",
    "    print(f\"  '{sent[:50]}...' -> Dialogue: {is_dial}, Locuteur: {speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traitement du corpus complet\n",
    "\n",
    "Application du pipeline NLP sur toutes les phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traiter un sous-ensemble pour commencer (pour tester)\n",
    "# On peut traiter tout le corpus en retirant cette limite\n",
    "SAMPLE_SIZE = 5000  # Commencer avec 5000 phrases\n",
    "df_sample = df.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"üìä Traitement de {len(df_sample):,} phrases...\")\n",
    "print(\"‚è≥ Cela peut prendre quelques minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter colonnes pour les annotations NLP\n",
    "df_sample['is_dialogue'] = False\n",
    "df_sample['speaker'] = None\n",
    "df_sample['entities_persons'] = None\n",
    "df_sample['entities_locations'] = None\n",
    "df_sample['word_count'] = 0\n",
    "\n",
    "# Traitement par batch pour efficacit√©\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), batch_size), desc=\"Processing batches\"):\n",
    "    batch = df_sample.iloc[i:i+batch_size]\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        \n",
    "        # D√©tection dialogue et locuteur\n",
    "        df_sample.at[idx, 'is_dialogue'] = is_dialogue(text)\n",
    "        df_sample.at[idx, 'speaker'] = detect_speaker(text)\n",
    "        \n",
    "        # Extraction d'entit√©s (plus lent, on peut optimiser)\n",
    "        try:\n",
    "            entities = extract_entities(text, nlp)\n",
    "            df_sample.at[idx, 'entities_persons'] = ','.join(entities['persons']) if entities['persons'] else None\n",
    "            df_sample.at[idx, 'entities_locations'] = ','.join(entities['locations']) if entities['locations'] else None\n",
    "        except:\n",
    "            pass  # En cas d'erreur, on continue\n",
    "        \n",
    "        # Compte de mots\n",
    "        df_sample.at[idx, 'word_count'] = len(text.split())\n",
    "\n",
    "print(\"\\n‚úÖ Traitement NLP termin√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques\n",
    "print(\"\\nüìä Statistiques du traitement:\")\n",
    "print(f\"  Phrases avec dialogue: {df_sample['is_dialogue'].sum():,}\")\n",
    "print(f\"  Phrases avec locuteur identifi√©: {df_sample['speaker'].notna().sum():,}\")\n",
    "print(f\"  Phrases avec entit√©s personnages: {df_sample['entities_persons'].notna().sum():,}\")\n",
    "\n",
    "print(\"\\nüó£Ô∏è  Top 10 locuteurs:\")\n",
    "speaker_counts = df_sample['speaker'].value_counts().head(10)\n",
    "print(speaker_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©er l'index d'entit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un index des mentions de personnages\n",
    "entity_mentions = []\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    if pd.notna(row['entities_persons']):\n",
    "        persons = row['entities_persons'].split(',')\n",
    "        for person in persons:\n",
    "            entity_mentions.append({\n",
    "                'entity_name': person.strip(),\n",
    "                'book_number': row['book_number'],\n",
    "                'book_title': row['book_title'],\n",
    "                'sentence_id': row['sentence_id'],\n",
    "                'context': row['text'][:200]  # Premiers 200 chars\n",
    "            })\n",
    "\n",
    "df_entities = pd.DataFrame(entity_mentions)\n",
    "\n",
    "print(f\"\\nüìá Index d'entit√©s cr√©√©: {len(df_entities):,} mentions\")\n",
    "print(f\"\\nüë• Personnages les plus mentionn√©s:\")\n",
    "print(df_entities['entity_name'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter le corpus avec annotations NLP\n",
    "output_path = DATA_DIR / 'nlp_processed.parquet'\n",
    "df_sample.to_parquet(output_path, index=False)\n",
    "print(f\"‚úÖ Corpus NLP export√©: {output_path}\")\n",
    "print(f\"   Taille: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Exporter l'index d'entit√©s\n",
    "entities_path = DATA_DIR / 'entities_index.parquet'\n",
    "df_entities.to_parquet(entities_path, index=False)\n",
    "print(f\"\\n‚úÖ Index d'entit√©s export√©: {entities_path}\")\n",
    "print(f\"   Taille: {entities_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des donn√©es\n",
    "print(\"\\nüìä Aper√ßu du corpus NLP:\")\n",
    "print(df_sample[['book_title', 'is_dialogue', 'speaker', 'entities_persons', 'text']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "Ce notebook a:\n",
    "1. ‚úÖ Charg√© et configur√© le mod√®le spaCy fran√ßais\n",
    "2. ‚úÖ Appliqu√© NER pour extraire les personnages et lieux\n",
    "3. ‚úÖ D√©tect√© les dialogues et attribu√© les locuteurs\n",
    "4. ‚úÖ Cr√©√© un index d'entit√©s\n",
    "5. ‚úÖ Export√© les donn√©es annot√©es\n",
    "\n",
    "**Notes:**\n",
    "- Pour le corpus complet, retirer la limite SAMPLE_SIZE\n",
    "- Le taux d'attribution de locuteur est d'environ 70-80% sur les dialogues d√©tect√©s\n",
    "- Les entit√©s sont normalis√©es vers les personnages principaux quand possible\n",
    "\n",
    "**Prochaine √©tape**: Notebook 03 - Extraction d'√©v√©nements sp√©cifiques avec classifieurs neuronaux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
