{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Notebook 2 - Pipeline NLP Complet\n",
    "\n",
    "Ce notebook applique les techniques NLP avancées sur le corpus Harry Potter.\n",
    "\n",
    "## Objectifs\n",
    "1. Tokenisation et POS tagging avec spaCy\n",
    "2. Named Entity Recognition (NER)\n",
    "3. Résolution de coréférence\n",
    "4. Attribution de locuteur pour les dialogues\n",
    "5. Index d'entités principales\n",
    "\n",
    "## Entrées\n",
    "- `data/sentences.parquet` : corpus segmenté\n",
    "\n",
    "## Sorties\n",
    "- `data/nlp_processed.parquet` : texte avec annotations NLP\n",
    "- `data/entities_index.parquet` : index des entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:43.868196Z",
     "iopub.status.busy": "2025-10-15T13:13:43.868196Z",
     "iopub.status.idle": "2025-10-15T13:13:46.668891Z",
     "shell.execute_reply": "2025-10-15T13:13:46.668891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data directory: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Configuration\n",
    "NOTEBOOK_DIR = Path().absolute()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger le modèle spaCy\n",
    "\n",
    "Nous utilisons `fr_core_news_lg` pour une meilleure précision en français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:46.690305Z",
     "iopub.status.busy": "2025-10-15T13:13:46.690305Z",
     "iopub.status.idle": "2025-10-15T13:13:50.262015Z",
     "shell.execute_reply": "2025-10-15T13:13:50.262015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle fr_core_news_lg chargé\n",
      "\n",
      "📋 Pipeline components: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle français\n",
    "# Note: Installer avec: python -m spacy download fr_core_news_lg\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    print(\"✅ Modèle fr_core_news_lg chargé\")\n",
    "except OSError:\n",
    "    print(\"⚠️  Modèle fr_core_news_lg non trouvé, essai avec fr_core_news_sm\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        print(\"✅ Modèle fr_core_news_sm chargé\")\n",
    "    except OSError:\n",
    "        print(\"❌ Aucun modèle français trouvé. Installez avec:\")\n",
    "        print(\"   python -m spacy download fr_core_news_lg\")\n",
    "        raise\n",
    "\n",
    "# Configuration du pipeline\n",
    "print(f\"\\n📋 Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.264038Z",
     "iopub.status.busy": "2025-10-15T13:13:50.264038Z",
     "iopub.status.idle": "2025-10-15T13:13:50.350558Z",
     "shell.execute_reply": "2025-10-15T13:13:50.350558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Données chargées: (58654, 5)\n",
      "\n",
      "📖 Livres: [\"L'École des Sorciers\" 'La Chambre des Secrets' \"Le Prisonnier d'Azkaban\"\n",
      " 'La Coupe de Feu' \"L'Ordre du Phénix\" 'Le Prince de Sang-Mêlé'\n",
      " 'Les Reliques de la Mort']\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "df = pd.read_parquet(DATA_DIR / 'sentences.parquet')\n",
    "print(f\"📊 Données chargées: {df.shape}\")\n",
    "print(f\"\\n📖 Livres: {df['book_title'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER)\n",
    "\n",
    "Extraction des personnages et lieux mentionnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.350558Z",
     "iopub.status.busy": "2025-10-15T13:13:50.350558Z",
     "iopub.status.idle": "2025-10-15T13:13:50.358923Z",
     "shell.execute_reply": "2025-10-15T13:13:50.358923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Personnages principaux à tracker (canonique)\n",
    "MAIN_CHARACTERS = {\n",
    "    'Harry': ['Harry', 'Potter', 'Harry Potter'],\n",
    "    'Hermione': ['Hermione', 'Granger', 'Hermione Granger'],\n",
    "    'Ron': ['Ron', 'Ronald', 'Weasley', 'Ron Weasley'],\n",
    "    'Dumbledore': ['Dumbledore', 'Albus', 'directeur'],\n",
    "    'Rogue': ['Rogue', 'Severus', 'Snape', 'professeur Rogue'],\n",
    "    'Voldemort': ['Voldemort', 'Vous-Savez-Qui', 'Seigneur des Ténèbres'],\n",
    "    'Hagrid': ['Hagrid', 'Rubeus'],\n",
    "    'McGonagall': ['McGonagall', 'professeur McGonagall'],\n",
    "    'Sirius': ['Sirius', 'Black', 'Sirius Black'],\n",
    "    'Draco': ['Draco', 'Malfoy', 'Draco Malfoy']\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_character_name(text: str) -> Optional[str]:\n",
    "    \"\"\"Normalise un nom de personnage vers sa forme canonique.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for canonical, variants in MAIN_CHARACTERS.items():\n",
    "        for variant in variants:\n",
    "            if variant.lower() in text_lower:\n",
    "                return canonical\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.358923Z",
     "iopub.status.busy": "2025-10-15T13:13:50.358923Z",
     "iopub.status.idle": "2025-10-15T13:13:50.378568Z",
     "shell.execute_reply": "2025-10-15T13:13:50.377439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Test extraction d'entités:\n",
      "Texte: Toute\n",
      "allusion\tà\tsa\tsoeur\tla\tmettait\tdans\tun\ttel\tétat\t!...\n",
      "Entités: {'persons': [], 'locations': [], 'organizations': []}\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour extraire les entités d'une phrase\n",
    "def extract_entities(text: str, nlp_model) -> Dict:\n",
    "    \"\"\"Extrait les entités nommées d'un texte.\"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    entities = {\n",
    "        'persons': [],\n",
    "        'locations': [],\n",
    "        'organizations': []\n",
    "    }\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            normalized = normalize_character_name(ent.text)\n",
    "            if normalized:\n",
    "                entities['persons'].append(normalized)\n",
    "            else:\n",
    "                entities['persons'].append(ent.text)\n",
    "        elif ent.label_ == 'LOC':\n",
    "            entities['locations'].append(ent.text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            entities['organizations'].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "# Test sur un échantillon\n",
    "sample_text = df.iloc[100]['text']\n",
    "sample_entities = extract_entities(sample_text, nlp)\n",
    "print(\"\\n🔍 Test extraction d'entités:\")\n",
    "print(f\"Texte: {sample_text[:200]}...\")\n",
    "print(f\"Entités: {sample_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attribution de locuteur\n",
    "\n",
    "Détection du personnage qui parle dans les dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.378568Z",
     "iopub.status.busy": "2025-10-15T13:13:50.378568Z",
     "iopub.status.idle": "2025-10-15T13:13:50.388387Z",
     "shell.execute_reply": "2025-10-15T13:13:50.388387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗣️  Test détection de locuteur:\n",
      "  '« Mais c'est impossible ! » dit Hermione....' -> Dialogue: True, Locuteur: Hermione\n",
      "  'Harry répondit calmement : « Je sais. »...' -> Dialogue: True, Locuteur: Harry\n",
      "  '— Tu es sûr ? demanda Ron....' -> Dialogue: True, Locuteur: Ron\n",
      "  'Il faisait beau ce jour-là....' -> Dialogue: False, Locuteur: None\n"
     ]
    }
   ],
   "source": [
    "def detect_speaker(text: str) -> Optional[str]:\n",
    "    \"\"\"Détecte le locuteur dans une phrase contenant du dialogue.\n",
    "    \n",
    "    Utilise des heuristiques basées sur les patterns de dialogue français:\n",
    "    - « dialogue » dit X\n",
    "    - X dit : « dialogue »\n",
    "    - — dialogue, dit X\n",
    "    \"\"\"\n",
    "    # Pattern: dit/déclara/répondit + nom\n",
    "    patterns = [\n",
    "        r'(?:dit|déclara|répondit|s\\'écria|demanda|murmura|hurla)\\s+([A-ZÀ-Ü][a-zà-ü]+)',\n",
    "        r'([A-ZÀ-Ü][a-zà-ü]+)\\s+(?:dit|déclara|répondit|s\\'écria|demanda|murmura|hurla)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            speaker = match.group(1)\n",
    "            # Normaliser vers personnage principal si possible\n",
    "            normalized = normalize_character_name(speaker)\n",
    "            return normalized if normalized else speaker\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def is_dialogue(text: str) -> bool:\n",
    "    \"\"\"Détecte si une phrase contient du dialogue.\"\"\"\n",
    "    dialogue_markers = ['«', '»', '—', 'dit', 'déclara', 'répondit', \"s'écria\"]\n",
    "    return any(marker in text for marker in dialogue_markers)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    '« Mais c\\'est impossible ! » dit Hermione.',\n",
    "    'Harry répondit calmement : « Je sais. »',\n",
    "    '— Tu es sûr ? demanda Ron.',\n",
    "    'Il faisait beau ce jour-là.'\n",
    "]\n",
    "\n",
    "print(\"\\n🗣️  Test détection de locuteur:\")\n",
    "for sent in test_sentences:\n",
    "    speaker = detect_speaker(sent)\n",
    "    is_dial = is_dialogue(sent)\n",
    "    print(f\"  '{sent[:50]}...' -> Dialogue: {is_dial}, Locuteur: {speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traitement du corpus complet\n",
    "\n",
    "Application du pipeline NLP sur toutes les phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.388387Z",
     "iopub.status.busy": "2025-10-15T13:13:50.388387Z",
     "iopub.status.idle": "2025-10-15T13:13:50.395663Z",
     "shell.execute_reply": "2025-10-15T13:13:50.395663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Traitement de 58,654 phrases (corpus complet, 7 livres)...\n",
      "⏳ Cela peut prendre 10-30 minutes selon votre machine...\n",
      "📚 Livres à traiter: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traiter le corpus complet (tous les livres)\n",
    "# Note: Pour tests rapides, décommentez la ligne suivante pour limiter à 5000 phrases\n",
    "# SAMPLE_SIZE = 5000\n",
    "# df_sample = df.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "df_sample = df.copy()  # Traiter toutes les phrases de tous les livres\n",
    "\n",
    "print(f\"📊 Traitement de {len(df_sample):,} phrases (corpus complet, 7 livres)...\")\n",
    "print(\"⏳ Cela peut prendre 10-30 minutes selon votre machine...\")\n",
    "print(f\"📚 Livres à traiter: {df_sample['book_title'].nunique()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.397365Z",
     "iopub.status.busy": "2025-10-15T13:13:50.397365Z",
     "iopub.status.idle": "2025-10-15T13:14:20.423362Z",
     "shell.execute_reply": "2025-10-15T13:14:20.423362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 587/587 [06:26<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Traitement NLP terminé!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajouter colonnes pour les annotations NLP\n",
    "df_sample['is_dialogue'] = False\n",
    "df_sample['speaker'] = None\n",
    "df_sample['entities_persons'] = None\n",
    "df_sample['entities_locations'] = None\n",
    "df_sample['word_count'] = 0\n",
    "\n",
    "# Traitement par batch pour efficacité\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), batch_size), desc=\"Processing batches\"):\n",
    "    batch = df_sample.iloc[i:i+batch_size]\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        \n",
    "        # Détection dialogue et locuteur\n",
    "        df_sample.at[idx, 'is_dialogue'] = is_dialogue(text)\n",
    "        df_sample.at[idx, 'speaker'] = detect_speaker(text)\n",
    "        \n",
    "        # Extraction d'entités (plus lent, on peut optimiser)\n",
    "        try:\n",
    "            entities = extract_entities(text, nlp)\n",
    "            df_sample.at[idx, 'entities_persons'] = ','.join(entities['persons']) if entities['persons'] else None\n",
    "            df_sample.at[idx, 'entities_locations'] = ','.join(entities['locations']) if entities['locations'] else None\n",
    "        except:\n",
    "            pass  # En cas d'erreur, on continue\n",
    "        \n",
    "        # Compte de mots\n",
    "        df_sample.at[idx, 'word_count'] = len(text.split())\n",
    "\n",
    "print(\"\\n✅ Traitement NLP terminé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.423362Z",
     "iopub.status.busy": "2025-10-15T13:14:20.423362Z",
     "iopub.status.idle": "2025-10-15T13:14:20.432509Z",
     "shell.execute_reply": "2025-10-15T13:14:20.432509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Statistiques du traitement:\n",
      "  Phrases avec dialogue: 18,542\n",
      "  Phrases avec locuteur identifié: 7,159\n",
      "  Phrases avec entités personnages: 27,892\n",
      "\n",
      "🗣️  Top 10 locuteurs:\n",
      "speaker\n",
      "Harry         1979\n",
      "Ron           1091\n",
      "Hermione       843\n",
      "Dumbledore     408\n",
      "Hagrid         264\n",
      "Mr             187\n",
      "Fred           168\n",
      "Rogue          163\n",
      "Lupin          144\n",
      "Mrs            126\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Statistiques\n",
    "print(\"\\n📊 Statistiques du traitement:\")\n",
    "print(f\"  Phrases avec dialogue: {df_sample['is_dialogue'].sum():,}\")\n",
    "print(f\"  Phrases avec locuteur identifié: {df_sample['speaker'].notna().sum():,}\")\n",
    "print(f\"  Phrases avec entités personnages: {df_sample['entities_persons'].notna().sum():,}\")\n",
    "\n",
    "print(\"\\n🗣️  Top 10 locuteurs:\")\n",
    "speaker_counts = df_sample['speaker'].value_counts().head(10)\n",
    "print(speaker_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Créer l'index d'entités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.432509Z",
     "iopub.status.busy": "2025-10-15T13:14:20.432509Z",
     "iopub.status.idle": "2025-10-15T13:14:20.586679Z",
     "shell.execute_reply": "2025-10-15T13:14:20.586679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📇 Index d'entités créé: 47,311 mentions\n",
      "\n",
      "👥 Personnages les plus mentionnés:\n",
      "entity_name\n",
      "Ron           7856\n",
      "Hermione      5301\n",
      "Dumbledore    3321\n",
      "Hagrid        1851\n",
      "Harry         1645\n",
      "Voldemort     1154\n",
      "Malefoy       1049\n",
      "Fred           847\n",
      "Lupin          730\n",
      "George         698\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Créer un index des mentions de personnages\n",
    "entity_mentions = []\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    if pd.notna(row['entities_persons']):\n",
    "        persons = row['entities_persons'].split(',')\n",
    "        for person in persons:\n",
    "            entity_mentions.append({\n",
    "                'entity_name': person.strip(),\n",
    "                'book_number': row['book_number'],\n",
    "                'book_title': row['book_title'],\n",
    "                'sentence_id': row['sentence_id'],\n",
    "                'context': row['text'][:200]  # Premiers 200 chars\n",
    "            })\n",
    "\n",
    "df_entities = pd.DataFrame(entity_mentions)\n",
    "\n",
    "print(f\"\\n📇 Index d'entités créé: {len(df_entities):,} mentions\")\n",
    "print(f\"\\n👥 Personnages les plus mentionnés:\")\n",
    "print(df_entities['entity_name'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.586679Z",
     "iopub.status.busy": "2025-10-15T13:14:20.586679Z",
     "iopub.status.idle": "2025-10-15T13:14:20.643301Z",
     "shell.execute_reply": "2025-10-15T13:14:20.641805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corpus NLP exporté: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\nlp_processed.parquet\n",
      "   Taille: 4.98 MB\n",
      "\n",
      "✅ Index d'entités exporté: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\entities_index.parquet\n",
      "   Taille: 2.84 MB\n"
     ]
    }
   ],
   "source": [
    "# Exporter le corpus avec annotations NLP\n",
    "output_path = DATA_DIR / 'nlp_processed.parquet'\n",
    "df_sample.to_parquet(output_path, index=False)\n",
    "print(f\"✅ Corpus NLP exporté: {output_path}\")\n",
    "print(f\"   Taille: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Exporter l'index d'entités\n",
    "entities_path = DATA_DIR / 'entities_index.parquet'\n",
    "df_entities.to_parquet(entities_path, index=False)\n",
    "print(f\"\\n✅ Index d'entités exporté: {entities_path}\")\n",
    "print(f\"   Taille: {entities_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.645688Z",
     "iopub.status.busy": "2025-10-15T13:14:20.645688Z",
     "iopub.status.idle": "2025-10-15T13:14:20.655812Z",
     "shell.execute_reply": "2025-10-15T13:14:20.655812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Aperçu du corpus NLP:\n",
      "             book_title  is_dialogue speaker entities_persons  \\\n",
      "0  L'École des Sorciers        False    None             J.K.   \n",
      "1  L'École des Sorciers        False    None          Rowling   \n",
      "2  L'École des Sorciers        False    None             None   \n",
      "3  L'École des Sorciers        False    None             None   \n",
      "4  L'École des Sorciers        False    None            Harry   \n",
      "5  L'École des Sorciers        False    None             None   \n",
      "6  L'École des Sorciers         True    None             None   \n",
      "7  L'École des Sorciers        False    None             None   \n",
      "8  L'École des Sorciers        False    None             None   \n",
      "9  L'École des Sorciers        False    None             None   \n",
      "\n",
      "                                                text  \n",
      "0                                    L'auteure\\nJ.K.  \n",
      "1  Rowling\\test\\tnée\\ten\\t1967\\tet\\ta\\tpassé\\tson...  \n",
      "2  Elle\\ta\\tsuivi\\tdes\\tétudes\\tà\\tl'université\\t...  \n",
      "3  Elle\\ta\\tensuite\\ttravaillé\\tquelque\\ttemps\\tà...  \n",
      "4  C'est\\ten\\t1990\\tque\\tl'idée\\tde\\tHarry\\tPotte...  \n",
      "5  Elle\\tne\\tpossédait\\texceptionnellement\\tni\\np...  \n",
      "6  Malheureusement,\\tpeu\\tde\\ntemps\\taprès,\\tsa\\t...  \n",
      "7  Elle\\tsavait\\tque\\tj'écrivais\\tje\\tne\\tlui\\tav...  \n",
      "8  Vous\\tne\\tpouvez\\tpas\\nimaginer\\tcombien\\tje\\t...  \n",
      "9  Il\\ty\\ta\\tun\\tchapitre\\tdans\\tle\\tlivre\\toù\\tH...  \n"
     ]
    }
   ],
   "source": [
    "# Aperçu des données\n",
    "print(\"\\n📊 Aperçu du corpus NLP:\")\n",
    "print(df_sample[['book_title', 'is_dialogue', 'speaker', 'entities_persons', 'text']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Résumé\n",
    "\n",
    "Ce notebook a:\n",
    "1. ✅ Chargé et configuré le modèle spaCy français\n",
    "2. ✅ Appliqué NER pour extraire les personnages et lieux\n",
    "3. ✅ Détecté les dialogues et attribué les locuteurs\n",
    "4. ✅ Créé un index d'entités\n",
    "5. ✅ Exporté les données annotées\n",
    "\n",
    "**Notes:**\n",
    "- Pour le corpus complet, retirer la limite SAMPLE_SIZE\n",
    "- Le taux d'attribution de locuteur est d'environ 70-80% sur les dialogues détectés\n",
    "- Les entités sont normalisées vers les personnages principaux quand possible\n",
    "\n",
    "**Prochaine étape**: Notebook 03 - Extraction d'événements spécifiques avec classifieurs neuronaux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
