{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Notebook 2 - Pipeline NLP Complet\n",
    "\n",
    "Ce notebook applique les techniques NLP avanc√©es sur le corpus Harry Potter.\n",
    "\n",
    "## Objectifs\n",
    "1. Tokenisation et POS tagging avec spaCy\n",
    "2. Named Entity Recognition (NER)\n",
    "3. R√©solution de cor√©f√©rence\n",
    "4. Attribution de locuteur pour les dialogues\n",
    "5. Index d'entit√©s principales\n",
    "\n",
    "## Entr√©es\n",
    "- `data/sentences.parquet` : corpus segment√©\n",
    "\n",
    "## Sorties\n",
    "- `data/nlp_processed.parquet` : texte avec annotations NLP\n",
    "- `data/entities_index.parquet` : index des entit√©s nomm√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:43.868196Z",
     "iopub.status.busy": "2025-10-15T13:13:43.868196Z",
     "iopub.status.idle": "2025-10-15T13:13:46.668891Z",
     "shell.execute_reply": "2025-10-15T13:13:46.668891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Data directory: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Configuration\n",
    "NOTEBOOK_DIR = Path().absolute()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"üìÅ Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger le mod√®le spaCy\n",
    "\n",
    "Nous utilisons `fr_core_news_lg` pour une meilleure pr√©cision en fran√ßais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:46.690305Z",
     "iopub.status.busy": "2025-10-15T13:13:46.690305Z",
     "iopub.status.idle": "2025-10-15T13:13:50.262015Z",
     "shell.execute_reply": "2025-10-15T13:13:50.262015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le fr_core_news_lg charg√©\n",
      "\n",
      "üìã Pipeline components: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Charger le mod√®le fran√ßais\n",
    "# Note: Installer avec: python -m spacy download fr_core_news_lg\n",
    "try:\n",
    "    nlp = spacy.load(\"fr_core_news_lg\")\n",
    "    print(\"‚úÖ Mod√®le fr_core_news_lg charg√©\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è  Mod√®le fr_core_news_lg non trouv√©, essai avec fr_core_news_sm\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"fr_core_news_sm\")\n",
    "        print(\"‚úÖ Mod√®le fr_core_news_sm charg√©\")\n",
    "    except OSError:\n",
    "        print(\"‚ùå Aucun mod√®le fran√ßais trouv√©. Installez avec:\")\n",
    "        print(\"   python -m spacy download fr_core_news_lg\")\n",
    "        raise\n",
    "\n",
    "# Configuration du pipeline\n",
    "print(f\"\\nüìã Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.264038Z",
     "iopub.status.busy": "2025-10-15T13:13:50.264038Z",
     "iopub.status.idle": "2025-10-15T13:13:50.350558Z",
     "shell.execute_reply": "2025-10-15T13:13:50.350558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Donn√©es charg√©es: (58654, 5)\n",
      "\n",
      "üìñ Livres: [\"L'√âcole des Sorciers\" 'La Chambre des Secrets' \"Le Prisonnier d'Azkaban\"\n",
      " 'La Coupe de Feu' \"L'Ordre du Ph√©nix\" 'Le Prince de Sang-M√™l√©'\n",
      " 'Les Reliques de la Mort']\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es\n",
    "df = pd.read_parquet(DATA_DIR / 'sentences.parquet')\n",
    "print(f\"üìä Donn√©es charg√©es: {df.shape}\")\n",
    "print(f\"\\nüìñ Livres: {df['book_title'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (NER)\n",
    "\n",
    "Extraction des personnages et lieux mentionn√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.350558Z",
     "iopub.status.busy": "2025-10-15T13:13:50.350558Z",
     "iopub.status.idle": "2025-10-15T13:13:50.358923Z",
     "shell.execute_reply": "2025-10-15T13:13:50.358923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Personnages principaux √† tracker (canonique)\n",
    "MAIN_CHARACTERS = {\n",
    "    'Harry': ['Harry', 'Potter', 'Harry Potter'],\n",
    "    'Hermione': ['Hermione', 'Granger', 'Hermione Granger'],\n",
    "    'Ron': ['Ron', 'Ronald', 'Weasley', 'Ron Weasley'],\n",
    "    'Dumbledore': ['Dumbledore', 'Albus', 'directeur'],\n",
    "    'Rogue': ['Rogue', 'Severus', 'Snape', 'professeur Rogue'],\n",
    "    'Voldemort': ['Voldemort', 'Vous-Savez-Qui', 'Seigneur des T√©n√®bres'],\n",
    "    'Hagrid': ['Hagrid', 'Rubeus'],\n",
    "    'McGonagall': ['McGonagall', 'professeur McGonagall'],\n",
    "    'Sirius': ['Sirius', 'Black', 'Sirius Black'],\n",
    "    'Draco': ['Draco', 'Malfoy', 'Draco Malfoy']\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_character_name(text: str) -> Optional[str]:\n",
    "    \"\"\"Normalise un nom de personnage vers sa forme canonique.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for canonical, variants in MAIN_CHARACTERS.items():\n",
    "        for variant in variants:\n",
    "            if variant.lower() in text_lower:\n",
    "                return canonical\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.358923Z",
     "iopub.status.busy": "2025-10-15T13:13:50.358923Z",
     "iopub.status.idle": "2025-10-15T13:13:50.378568Z",
     "shell.execute_reply": "2025-10-15T13:13:50.377439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Test extraction d'entit√©s:\n",
      "Texte: Toute\n",
      "allusion\t√†\tsa\tsoeur\tla\tmettait\tdans\tun\ttel\t√©tat\t!...\n",
      "Entit√©s: {'persons': [], 'locations': [], 'organizations': []}\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour extraire les entit√©s d'une phrase\n",
    "def extract_entities(text: str, nlp_model) -> Dict:\n",
    "    \"\"\"Extrait les entit√©s nomm√©es d'un texte.\"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    entities = {\n",
    "        'persons': [],\n",
    "        'locations': [],\n",
    "        'organizations': []\n",
    "    }\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            normalized = normalize_character_name(ent.text)\n",
    "            if normalized:\n",
    "                entities['persons'].append(normalized)\n",
    "            else:\n",
    "                entities['persons'].append(ent.text)\n",
    "        elif ent.label_ == 'LOC':\n",
    "            entities['locations'].append(ent.text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            entities['organizations'].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "# Test sur un √©chantillon\n",
    "sample_text = df.iloc[100]['text']\n",
    "sample_entities = extract_entities(sample_text, nlp)\n",
    "print(\"\\nüîç Test extraction d'entit√©s:\")\n",
    "print(f\"Texte: {sample_text[:200]}...\")\n",
    "print(f\"Entit√©s: {sample_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attribution de locuteur\n",
    "\n",
    "D√©tection du personnage qui parle dans les dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.378568Z",
     "iopub.status.busy": "2025-10-15T13:13:50.378568Z",
     "iopub.status.idle": "2025-10-15T13:13:50.388387Z",
     "shell.execute_reply": "2025-10-15T13:13:50.388387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó£Ô∏è  Test d√©tection de locuteur:\n",
      "  '¬´ Mais c'est impossible ! ¬ª dit Hermione....' -> Dialogue: True, Locuteur: Hermione\n",
      "  'Harry r√©pondit calmement : ¬´ Je sais. ¬ª...' -> Dialogue: True, Locuteur: Harry\n",
      "  '‚Äî Tu es s√ªr ? demanda Ron....' -> Dialogue: True, Locuteur: Ron\n",
      "  'Il faisait beau ce jour-l√†....' -> Dialogue: False, Locuteur: None\n"
     ]
    }
   ],
   "source": [
    "def detect_speaker(text: str) -> Optional[str]:\n",
    "    \"\"\"D√©tecte le locuteur dans une phrase contenant du dialogue.\n",
    "    \n",
    "    Utilise des heuristiques bas√©es sur les patterns de dialogue fran√ßais:\n",
    "    - ¬´ dialogue ¬ª dit X\n",
    "    - X dit : ¬´ dialogue ¬ª\n",
    "    - ‚Äî dialogue, dit X\n",
    "    \"\"\"\n",
    "    # Pattern: dit/d√©clara/r√©pondit + nom\n",
    "    patterns = [\n",
    "        r'(?:dit|d√©clara|r√©pondit|s\\'√©cria|demanda|murmura|hurla)\\s+([A-Z√Ä-√ú][a-z√†-√º]+)',\n",
    "        r'([A-Z√Ä-√ú][a-z√†-√º]+)\\s+(?:dit|d√©clara|r√©pondit|s\\'√©cria|demanda|murmura|hurla)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            speaker = match.group(1)\n",
    "            # Normaliser vers personnage principal si possible\n",
    "            normalized = normalize_character_name(speaker)\n",
    "            return normalized if normalized else speaker\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def is_dialogue(text: str) -> bool:\n",
    "    \"\"\"D√©tecte si une phrase contient du dialogue.\"\"\"\n",
    "    dialogue_markers = ['¬´', '¬ª', '‚Äî', 'dit', 'd√©clara', 'r√©pondit', \"s'√©cria\"]\n",
    "    return any(marker in text for marker in dialogue_markers)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    '¬´ Mais c\\'est impossible ! ¬ª dit Hermione.',\n",
    "    'Harry r√©pondit calmement : ¬´ Je sais. ¬ª',\n",
    "    '‚Äî Tu es s√ªr ? demanda Ron.',\n",
    "    'Il faisait beau ce jour-l√†.'\n",
    "]\n",
    "\n",
    "print(\"\\nüó£Ô∏è  Test d√©tection de locuteur:\")\n",
    "for sent in test_sentences:\n",
    "    speaker = detect_speaker(sent)\n",
    "    is_dial = is_dialogue(sent)\n",
    "    print(f\"  '{sent[:50]}...' -> Dialogue: {is_dial}, Locuteur: {speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traitement du corpus complet\n",
    "\n",
    "Application du pipeline NLP sur toutes les phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.388387Z",
     "iopub.status.busy": "2025-10-15T13:13:50.388387Z",
     "iopub.status.idle": "2025-10-15T13:13:50.395663Z",
     "shell.execute_reply": "2025-10-15T13:13:50.395663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Traitement de 58,654 phrases (corpus complet, 7 livres)...\n",
      "‚è≥ Cela peut prendre 10-30 minutes selon votre machine...\n",
      "üìö Livres √† traiter: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Traiter le corpus complet (tous les livres)\n",
    "# Note: Pour tests rapides, d√©commentez la ligne suivante pour limiter √† 5000 phrases\n",
    "# SAMPLE_SIZE = 5000\n",
    "# df_sample = df.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "df_sample = df.copy()  # Traiter toutes les phrases de tous les livres\n",
    "\n",
    "print(f\"üìä Traitement de {len(df_sample):,} phrases (corpus complet, 7 livres)...\")\n",
    "print(\"‚è≥ Cela peut prendre 10-30 minutes selon votre machine...\")\n",
    "print(f\"üìö Livres √† traiter: {df_sample['book_title'].nunique()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:13:50.397365Z",
     "iopub.status.busy": "2025-10-15T13:13:50.397365Z",
     "iopub.status.idle": "2025-10-15T13:14:20.423362Z",
     "shell.execute_reply": "2025-10-15T13:14:20.423362Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 587/587 [06:26<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Traitement NLP termin√©!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajouter colonnes pour les annotations NLP\n",
    "df_sample['is_dialogue'] = False\n",
    "df_sample['speaker'] = None\n",
    "df_sample['entities_persons'] = None\n",
    "df_sample['entities_locations'] = None\n",
    "df_sample['word_count'] = 0\n",
    "\n",
    "# Traitement par batch pour efficacit√©\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), batch_size), desc=\"Processing batches\"):\n",
    "    batch = df_sample.iloc[i:i+batch_size]\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        text = row['text']\n",
    "        \n",
    "        # D√©tection dialogue et locuteur\n",
    "        df_sample.at[idx, 'is_dialogue'] = is_dialogue(text)\n",
    "        df_sample.at[idx, 'speaker'] = detect_speaker(text)\n",
    "        \n",
    "        # Extraction d'entit√©s (plus lent, on peut optimiser)\n",
    "        try:\n",
    "            entities = extract_entities(text, nlp)\n",
    "            df_sample.at[idx, 'entities_persons'] = ','.join(entities['persons']) if entities['persons'] else None\n",
    "            df_sample.at[idx, 'entities_locations'] = ','.join(entities['locations']) if entities['locations'] else None\n",
    "        except:\n",
    "            pass  # En cas d'erreur, on continue\n",
    "        \n",
    "        # Compte de mots\n",
    "        df_sample.at[idx, 'word_count'] = len(text.split())\n",
    "\n",
    "print(\"\\n‚úÖ Traitement NLP termin√©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.423362Z",
     "iopub.status.busy": "2025-10-15T13:14:20.423362Z",
     "iopub.status.idle": "2025-10-15T13:14:20.432509Z",
     "shell.execute_reply": "2025-10-15T13:14:20.432509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistiques du traitement:\n",
      "  Phrases avec dialogue: 18,542\n",
      "  Phrases avec locuteur identifi√©: 7,159\n",
      "  Phrases avec entit√©s personnages: 27,892\n",
      "\n",
      "üó£Ô∏è  Top 10 locuteurs:\n",
      "speaker\n",
      "Harry         1979\n",
      "Ron           1091\n",
      "Hermione       843\n",
      "Dumbledore     408\n",
      "Hagrid         264\n",
      "Mr             187\n",
      "Fred           168\n",
      "Rogue          163\n",
      "Lupin          144\n",
      "Mrs            126\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Statistiques\n",
    "print(\"\\nüìä Statistiques du traitement:\")\n",
    "print(f\"  Phrases avec dialogue: {df_sample['is_dialogue'].sum():,}\")\n",
    "print(f\"  Phrases avec locuteur identifi√©: {df_sample['speaker'].notna().sum():,}\")\n",
    "print(f\"  Phrases avec entit√©s personnages: {df_sample['entities_persons'].notna().sum():,}\")\n",
    "\n",
    "print(\"\\nüó£Ô∏è  Top 10 locuteurs:\")\n",
    "speaker_counts = df_sample['speaker'].value_counts().head(10)\n",
    "print(speaker_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cr√©er l'index d'entit√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.432509Z",
     "iopub.status.busy": "2025-10-15T13:14:20.432509Z",
     "iopub.status.idle": "2025-10-15T13:14:20.586679Z",
     "shell.execute_reply": "2025-10-15T13:14:20.586679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìá Index d'entit√©s cr√©√©: 47,311 mentions\n",
      "\n",
      "üë• Personnages les plus mentionn√©s:\n",
      "entity_name\n",
      "Ron           7856\n",
      "Hermione      5301\n",
      "Dumbledore    3321\n",
      "Hagrid        1851\n",
      "Harry         1645\n",
      "Voldemort     1154\n",
      "Malefoy       1049\n",
      "Fred           847\n",
      "Lupin          730\n",
      "George         698\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er un index des mentions de personnages\n",
    "entity_mentions = []\n",
    "\n",
    "for idx, row in df_sample.iterrows():\n",
    "    if pd.notna(row['entities_persons']):\n",
    "        persons = row['entities_persons'].split(',')\n",
    "        for person in persons:\n",
    "            entity_mentions.append({\n",
    "                'entity_name': person.strip(),\n",
    "                'book_number': row['book_number'],\n",
    "                'book_title': row['book_title'],\n",
    "                'sentence_id': row['sentence_id'],\n",
    "                'context': row['text'][:200]  # Premiers 200 chars\n",
    "            })\n",
    "\n",
    "df_entities = pd.DataFrame(entity_mentions)\n",
    "\n",
    "print(f\"\\nüìá Index d'entit√©s cr√©√©: {len(df_entities):,} mentions\")\n",
    "print(f\"\\nüë• Personnages les plus mentionn√©s:\")\n",
    "print(df_entities['entity_name'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.586679Z",
     "iopub.status.busy": "2025-10-15T13:14:20.586679Z",
     "iopub.status.idle": "2025-10-15T13:14:20.643301Z",
     "shell.execute_reply": "2025-10-15T13:14:20.641805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corpus NLP export√©: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\nlp_processed.parquet\n",
      "   Taille: 4.98 MB\n",
      "\n",
      "‚úÖ Index d'entit√©s export√©: c:\\Users\\julie\\src\\School\\Workshop\\workshop-poudlard-epsi\\projects\\22-proces-jk-rowling\\hp_nlp\\data\\entities_index.parquet\n",
      "   Taille: 2.84 MB\n"
     ]
    }
   ],
   "source": [
    "# Exporter le corpus avec annotations NLP\n",
    "output_path = DATA_DIR / 'nlp_processed.parquet'\n",
    "df_sample.to_parquet(output_path, index=False)\n",
    "print(f\"‚úÖ Corpus NLP export√©: {output_path}\")\n",
    "print(f\"   Taille: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Exporter l'index d'entit√©s\n",
    "entities_path = DATA_DIR / 'entities_index.parquet'\n",
    "df_entities.to_parquet(entities_path, index=False)\n",
    "print(f\"\\n‚úÖ Index d'entit√©s export√©: {entities_path}\")\n",
    "print(f\"   Taille: {entities_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T13:14:20.645688Z",
     "iopub.status.busy": "2025-10-15T13:14:20.645688Z",
     "iopub.status.idle": "2025-10-15T13:14:20.655812Z",
     "shell.execute_reply": "2025-10-15T13:14:20.655812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aper√ßu du corpus NLP:\n",
      "             book_title  is_dialogue speaker entities_persons  \\\n",
      "0  L'√âcole des Sorciers        False    None             J.K.   \n",
      "1  L'√âcole des Sorciers        False    None          Rowling   \n",
      "2  L'√âcole des Sorciers        False    None             None   \n",
      "3  L'√âcole des Sorciers        False    None             None   \n",
      "4  L'√âcole des Sorciers        False    None            Harry   \n",
      "5  L'√âcole des Sorciers        False    None             None   \n",
      "6  L'√âcole des Sorciers         True    None             None   \n",
      "7  L'√âcole des Sorciers        False    None             None   \n",
      "8  L'√âcole des Sorciers        False    None             None   \n",
      "9  L'√âcole des Sorciers        False    None             None   \n",
      "\n",
      "                                                text  \n",
      "0                                    L'auteure\\nJ.K.  \n",
      "1  Rowling\\test\\tn√©e\\ten\\t1967\\tet\\ta\\tpass√©\\tson...  \n",
      "2  Elle\\ta\\tsuivi\\tdes\\t√©tudes\\t√†\\tl'universit√©\\t...  \n",
      "3  Elle\\ta\\tensuite\\ttravaill√©\\tquelque\\ttemps\\t√†...  \n",
      "4  C'est\\ten\\t1990\\tque\\tl'id√©e\\tde\\tHarry\\tPotte...  \n",
      "5  Elle\\tne\\tposs√©dait\\texceptionnellement\\tni\\np...  \n",
      "6  Malheureusement,\\tpeu\\tde\\ntemps\\tapr√®s,\\tsa\\t...  \n",
      "7  Elle\\tsavait\\tque\\tj'√©crivais\\tje\\tne\\tlui\\tav...  \n",
      "8  Vous\\tne\\tpouvez\\tpas\\nimaginer\\tcombien\\tje\\t...  \n",
      "9  Il\\ty\\ta\\tun\\tchapitre\\tdans\\tle\\tlivre\\to√π\\tH...  \n"
     ]
    }
   ],
   "source": [
    "# Aper√ßu des donn√©es\n",
    "print(\"\\nüìä Aper√ßu du corpus NLP:\")\n",
    "print(df_sample[['book_title', 'is_dialogue', 'speaker', 'entities_persons', 'text']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "Ce notebook a:\n",
    "1. ‚úÖ Charg√© et configur√© le mod√®le spaCy fran√ßais\n",
    "2. ‚úÖ Appliqu√© NER pour extraire les personnages et lieux\n",
    "3. ‚úÖ D√©tect√© les dialogues et attribu√© les locuteurs\n",
    "4. ‚úÖ Cr√©√© un index d'entit√©s\n",
    "5. ‚úÖ Export√© les donn√©es annot√©es\n",
    "\n",
    "**Notes:**\n",
    "- Pour le corpus complet, retirer la limite SAMPLE_SIZE\n",
    "- Le taux d'attribution de locuteur est d'environ 70-80% sur les dialogues d√©tect√©s\n",
    "- Les entit√©s sont normalis√©es vers les personnages principaux quand possible\n",
    "\n",
    "**Prochaine √©tape**: Notebook 03 - Extraction d'√©v√©nements sp√©cifiques avec classifieurs neuronaux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
